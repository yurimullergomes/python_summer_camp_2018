{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names:  Yuri Müller Plumm (CBPF)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Monte Carlo Simulation\n",
    "df_ee = pd.read_hdf('z0_mc_ee.h5')\n",
    "df_mm = pd.read_hdf('z0_mc_mm.h5')\n",
    "df_tt = pd.read_hdf('z0_mc_tt.h5')\n",
    "df_qq = pd.read_hdf('z0_mc_qq.h5')\n",
    "\n",
    "#Data\n",
    "df_data1 = pd.read_hdf('z0_data1.h5')\n",
    "df_data2 = pd.read_hdf('z0_data2.h5')\n",
    "df_data3 = pd.read_hdf('z0_data3.h5')\n",
    "df_data4 = pd.read_hdf('z0_data4.h5')\n",
    "df_data5 = pd.read_hdf('z0_data5.h5')\n",
    "df_data6 = pd.read_hdf('z0_data6.h5')\n",
    "df_data7 = pd.read_hdf('z0_data7.h5')\n",
    "\n",
    "#Defining each channel, using the monte carlo of each channel\n",
    "lista_0 = [0 for i in range(100000)]\n",
    "class_ee = pd.DataFrame({'class':lista_0})\n",
    "lista_1 = [1 for i in range(100000)]\n",
    "class_mm = pd.DataFrame({'class':lista_1})\n",
    "lista_2 = [2 for i in range(100000)]\n",
    "class_tt = pd.DataFrame({'class':lista_2})\n",
    "lista_3 = [3 for i in range(100000)]\n",
    "class_qq = pd.DataFrame({'class':lista_3})\n",
    "\n",
    "df_ee_final = df_ee.join(class_ee)\n",
    "df_mm_final = df_mm.join(class_mm)\n",
    "df_tt_final = df_tt.join(class_tt)\n",
    "df_qq_final = df_qq.join(class_qq)\n",
    "\n",
    "#Taking off some non-prhisical feature of the data that can confuse the NN\n",
    "result1 = pd.concat([df_ee_final,df_mm_final,df_tt_final,df_qq_final])\n",
    "result0 = result1.drop(['run','event','e_lep'],axis=1)\n",
    "#result.head()\n",
    "\n",
    "df_data1_neww = df_data1.drop(['run','event','e_lep'],axis=1)\n",
    "df_data2_neww = df_data2.drop(['run','event','e_lep'],axis=1)\n",
    "df_data3_neww = df_data3.drop(['run','event','e_lep'],axis=1)\n",
    "df_data4_neww = df_data4.drop(['run','event','e_lep'],axis=1)\n",
    "df_data5_neww = df_data5.drop(['run','event','e_lep'],axis=1)\n",
    "df_data6_neww = df_data6.drop(['run','event','e_lep'],axis=1)\n",
    "df_data7_neww = df_data7.drop(['run','event','e_lep'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = ['Electron','Muon','Tau','Quark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acol</th>\n",
       "      <th>cos_thrust</th>\n",
       "      <th>cos_thrust_neg</th>\n",
       "      <th>cos_thrust_pos</th>\n",
       "      <th>d0_mean</th>\n",
       "      <th>e_ecal</th>\n",
       "      <th>e_hcal</th>\n",
       "      <th>n_charged</th>\n",
       "      <th>n_ecal</th>\n",
       "      <th>n_muons</th>\n",
       "      <th>p_charged</th>\n",
       "      <th>phi_thrust</th>\n",
       "      <th>thrust</th>\n",
       "      <th>z0_mean</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.478470</td>\n",
       "      <td>-0.857441</td>\n",
       "      <td>-0.860064</td>\n",
       "      <td>0.855789</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>88.929619</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>81.328445</td>\n",
       "      <td>-118.945854</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>-0.114538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.073654</td>\n",
       "      <td>-0.360697</td>\n",
       "      <td>-0.361289</td>\n",
       "      <td>0.360372</td>\n",
       "      <td>0.026090</td>\n",
       "      <td>90.303406</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>71.335449</td>\n",
       "      <td>74.006271</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.052427</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.383408</td>\n",
       "      <td>-0.971304</td>\n",
       "      <td>-0.970553</td>\n",
       "      <td>0.971979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.632736</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.534874</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.977844</td>\n",
       "      <td>0.420864</td>\n",
       "      <td>0.468641</td>\n",
       "      <td>-0.395415</td>\n",
       "      <td>0.023564</td>\n",
       "      <td>90.568001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>62.687538</td>\n",
       "      <td>35.988522</td>\n",
       "      <td>0.963187</td>\n",
       "      <td>0.694042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.137634</td>\n",
       "      <td>-0.968132</td>\n",
       "      <td>-0.968279</td>\n",
       "      <td>0.967988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.347679</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-155.903839</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acol  cos_thrust  cos_thrust_neg  cos_thrust_pos   d0_mean     e_ecal  \\\n",
       "0  0.478470   -0.857441       -0.860064        0.855789  0.012637  88.929619   \n",
       "1  0.073654   -0.360697       -0.361289        0.360372  0.026090  90.303406   \n",
       "2  0.383408   -0.971304       -0.970553        0.971979  0.000000  89.632736   \n",
       "3  8.977844    0.420864        0.468641       -0.395415  0.023564  90.568001   \n",
       "4  0.137634   -0.968132       -0.968279        0.967988  0.000000  84.347679   \n",
       "\n",
       "   e_hcal  n_charged  n_ecal  n_muons  p_charged  phi_thrust    thrust  \\\n",
       "0    0.00          2       2        0  81.328445 -118.945854  0.999992   \n",
       "1    0.00          2       2        0  71.335449   74.006271  1.000000   \n",
       "2    3.19          0       2        0   0.000000 -100.534874  0.999994   \n",
       "3    0.00          2       3        0  62.687538   35.988522  0.963187   \n",
       "4    2.10          0       2        0   0.000000 -155.903839  0.999999   \n",
       "\n",
       "    z0_mean  class  \n",
       "0 -0.114538      0  \n",
       "1 -0.052427      0  \n",
       "2  0.000000      0  \n",
       "3  0.694042      0  \n",
       "4  0.000000      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result0.sample(frac=1).reset_index(drop=True)  #shuffle the rows only\n",
    "#Xclass = result.drop('class',axis=0)\n",
    "#yclass = result['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xclass = result.drop('class',axis=1)\n",
    "yclass = result['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w8FWxMTyeS6EgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b261dad278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yclass.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = np.array(Xclass)\n",
    "y_array = np.array(yclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(y_array)#total number of simulations\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "def creating_training_data():\n",
    "    for i in range(int(N)):  # do dogs and cats\n",
    "        try:\n",
    "            event = X_array[i]  # convert to array\n",
    "            tipo = y_array[i]\n",
    "            training_data.append([event,tipo])\n",
    "        except Exception as e:   #I should put some warning here\n",
    "                pass\n",
    "\n",
    "creating_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2.64923096e-01, -8.23731348e-02, -8.01724195e-02,  8.47266018e-02,\n",
       "         5.16463108e-02,  8.47425690e+01,  8.14385319e+00,  2.00000000e+00,\n",
       "         3.00000000e+00,  0.00000000e+00,  9.14197083e+01, -5.21306915e+01,\n",
       "         9.99928236e-01,  1.86932099e+00]), 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for imgs,types in training_data:\n",
    "    \n",
    "    y.append(types)\n",
    "    X.append(imgs)\n",
    "\n",
    "print(np.size(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"XLEP.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"yLEP.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data saved. Now we could construct our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # for a sequential model \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import os\n",
    "#import cv2\n",
    "#from tqdm import tqdm\n",
    "#import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_in = open(\"XLEP.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"yLEP.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "X = np.array(X) #normalizing data\n",
    "\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 10\n",
    "X_test = np.array([X[i] for i in range((len(X)//df))])\n",
    "X_train = np.array([X[i] for i in range((len(X)//df),len(X))])\n",
    "\n",
    "y_test = np.array([y[i] for i in range((len(y)//df))])\n",
    "y_train = np.array([y[i] for i in range((len(y)//df),len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.11159266,  0.09432856,  0.12953   ,  0.13217645, -0.01146428,\n",
       "         1.27495276, -0.19179644, -0.54690403, -0.50958377, -0.61716053,\n",
       "         0.01204416, -0.76040616,  0.09634945,  0.74959191]), 14)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0],len(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refazer a análise abaixo com os dados tratados corretamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEP-0.1-dr-32-l-1-de-1537466802\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 15s 61us/step - loss: 0.1328 - acc: 0.9532 - val_loss: 0.0626 - val_acc: 0.9780\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 14s 57us/step - loss: 0.0666 - acc: 0.9764 - val_loss: 0.0537 - val_acc: 0.9802\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 13s 52us/step - loss: 0.0583 - acc: 0.9788 - val_loss: 0.0443 - val_acc: 0.9833\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 15s 58us/step - loss: 0.0539 - acc: 0.9801 - val_loss: 0.0425 - val_acc: 0.9845\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 14s 57us/step - loss: 0.0509 - acc: 0.9811 - val_loss: 0.0412 - val_acc: 0.9841\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 15s 60us/step - loss: 0.0488 - acc: 0.9820 - val_loss: 0.0388 - val_acc: 0.9850\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 14s 57us/step - loss: 0.0477 - acc: 0.9825 - val_loss: 0.0395 - val_acc: 0.9853\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 17s 67us/step - loss: 0.0464 - acc: 0.9827 - val_loss: 0.0377 - val_acc: 0.9859\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 19s 74us/step - loss: 0.0458 - acc: 0.9830 - val_loss: 0.0396 - val_acc: 0.9852\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 13s 53us/step - loss: 0.0448 - acc: 0.9833 - val_loss: 0.0371 - val_acc: 0.9864\n",
      "LEP-0.3-dr-32-l-1-de-1537466958\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 14s 55us/step - loss: 0.1831 - acc: 0.9351 - val_loss: 0.0694 - val_acc: 0.9740\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 16s 62us/step - loss: 0.0873 - acc: 0.9705 - val_loss: 0.0563 - val_acc: 0.9792\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 15s 59us/step - loss: 0.0757 - acc: 0.9740 - val_loss: 0.0527 - val_acc: 0.9797\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 14s 54us/step - loss: 0.0708 - acc: 0.9752 - val_loss: 0.0472 - val_acc: 0.9831\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 14s 58us/step - loss: 0.0666 - acc: 0.9767 - val_loss: 0.0462 - val_acc: 0.9825\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 12s 49us/step - loss: 0.0642 - acc: 0.9777 - val_loss: 0.0443 - val_acc: 0.9844\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 14s 54us/step - loss: 0.0618 - acc: 0.9788 - val_loss: 0.0437 - val_acc: 0.9841\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 12s 50us/step - loss: 0.0614 - acc: 0.9782 - val_loss: 0.0428 - val_acc: 0.9845\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0616 - acc: 0.9786 - val_loss: 0.0430 - val_acc: 0.9844\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 16s 64us/step - loss: 0.0614 - acc: 0.9787 - val_loss: 0.0437 - val_acc: 0.9837\n",
      "LEP-0.5-dr-32-l-1-de-1537467103\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 15s 58us/step - loss: 0.2495 - acc: 0.9110 - val_loss: 0.0897 - val_acc: 0.9732\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 13s 51us/step - loss: 0.1328 - acc: 0.9579 - val_loss: 0.0672 - val_acc: 0.9745\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 13s 52us/step - loss: 0.1161 - acc: 0.9626 - val_loss: 0.0619 - val_acc: 0.9786\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 14s 55us/step - loss: 0.1081 - acc: 0.9654 - val_loss: 0.0566 - val_acc: 0.9801\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 12s 46us/step - loss: 0.1064 - acc: 0.9669 - val_loss: 0.0567 - val_acc: 0.9809\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 11s 42us/step - loss: 0.1033 - acc: 0.9666 - val_loss: 0.0562 - val_acc: 0.9807\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 11s 43us/step - loss: 0.0999 - acc: 0.9683 - val_loss: 0.0539 - val_acc: 0.9808\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 16s 62us/step - loss: 0.0982 - acc: 0.9687 - val_loss: 0.0548 - val_acc: 0.9798\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 18s 72us/step - loss: 0.0981 - acc: 0.9674 - val_loss: 0.0532 - val_acc: 0.9801\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 16s 64us/step - loss: 0.0972 - acc: 0.9676 - val_loss: 0.0530 - val_acc: 0.9804\n",
      "LEP-0.1-dr-64-l-1-de-1537467241\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 16s 63us/step - loss: 0.1118 - acc: 0.9607 - val_loss: 0.0555 - val_acc: 0.9778\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 17s 67us/step - loss: 0.0558 - acc: 0.9792 - val_loss: 0.0455 - val_acc: 0.9831\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 16s 64us/step - loss: 0.0499 - acc: 0.9814 - val_loss: 0.0425 - val_acc: 0.9846\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 18s 73us/step - loss: 0.0458 - acc: 0.9826 - val_loss: 0.0395 - val_acc: 0.9844\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 15s 60us/step - loss: 0.0444 - acc: 0.9834 - val_loss: 0.0391 - val_acc: 0.9854\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 14s 55us/step - loss: 0.0432 - acc: 0.9838 - val_loss: 0.0368 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 13s 54us/step - loss: 0.0417 - acc: 0.9841 - val_loss: 0.0360 - val_acc: 0.9868\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 16s 62us/step - loss: 0.0408 - acc: 0.9845 - val_loss: 0.0378 - val_acc: 0.9863\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 16s 62us/step - loss: 0.0406 - acc: 0.9848 - val_loss: 0.0372 - val_acc: 0.9859\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 15s 61us/step - loss: 0.0397 - acc: 0.9850 - val_loss: 0.0347 - val_acc: 0.9868\n",
      "LEP-0.3-dr-64-l-1-de-1537467399\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 14s 54us/step - loss: 0.1276 - acc: 0.9556 - val_loss: 0.0556 - val_acc: 0.9797\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 13s 50us/step - loss: 0.0658 - acc: 0.9766 - val_loss: 0.0461 - val_acc: 0.9826\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 14s 54us/step - loss: 0.0588 - acc: 0.9785 - val_loss: 0.0430 - val_acc: 0.9840\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 15s 59us/step - loss: 0.0555 - acc: 0.9800 - val_loss: 0.0418 - val_acc: 0.9849\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 14s 57us/step - loss: 0.0535 - acc: 0.9805 - val_loss: 0.0411 - val_acc: 0.9853\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 16s 62us/step - loss: 0.0520 - acc: 0.9810 - val_loss: 0.0387 - val_acc: 0.9859\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 15s 58us/step - loss: 0.0511 - acc: 0.9813 - val_loss: 0.0394 - val_acc: 0.9856\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 15s 60us/step - loss: 0.0499 - acc: 0.9820 - val_loss: 0.0385 - val_acc: 0.9858\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 13s 51us/step - loss: 0.0488 - acc: 0.9822 - val_loss: 0.0374 - val_acc: 0.9860\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 13s 52us/step - loss: 0.0490 - acc: 0.9819 - val_loss: 0.0369 - val_acc: 0.9858\n",
      "LEP-0.5-dr-64-l-1-de-1537467541\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 14s 54us/step - loss: 0.1790 - acc: 0.9378 - val_loss: 0.0724 - val_acc: 0.9748\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 13s 52us/step - loss: 0.0911 - acc: 0.9706 - val_loss: 0.0545 - val_acc: 0.9802\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 13s 52us/step - loss: 0.0774 - acc: 0.9745 - val_loss: 0.0513 - val_acc: 0.9820\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 14s 57us/step - loss: 0.0720 - acc: 0.9757 - val_loss: 0.0488 - val_acc: 0.9819\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 14s 55us/step - loss: 0.0701 - acc: 0.9761 - val_loss: 0.0462 - val_acc: 0.9836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 14s 57us/step - loss: 0.0680 - acc: 0.9764 - val_loss: 0.0448 - val_acc: 0.9828\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 14s 55us/step - loss: 0.0672 - acc: 0.9768 - val_loss: 0.0458 - val_acc: 0.9840\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 14s 57us/step - loss: 0.0657 - acc: 0.9773 - val_loss: 0.0455 - val_acc: 0.9836\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 15s 58us/step - loss: 0.0663 - acc: 0.9775 - val_loss: 0.0442 - val_acc: 0.9839\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 14s 54us/step - loss: 0.0641 - acc: 0.9782 - val_loss: 0.0457 - val_acc: 0.9836\n",
      "LEP-0.1-dr-128-l-1-de-1537467680\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 18s 73us/step - loss: 0.0892 - acc: 0.9683 - val_loss: 0.0499 - val_acc: 0.9815\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 17s 67us/step - loss: 0.0519 - acc: 0.9806 - val_loss: 0.0443 - val_acc: 0.9829\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 18s 73us/step - loss: 0.0467 - acc: 0.9822 - val_loss: 0.0395 - val_acc: 0.9853\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 18s 73us/step - loss: 0.0438 - acc: 0.9834 - val_loss: 0.0402 - val_acc: 0.9855\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 20s 78us/step - loss: 0.0422 - acc: 0.9842 - val_loss: 0.0381 - val_acc: 0.9861\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 21s 84us/step - loss: 0.0410 - acc: 0.9845 - val_loss: 0.0379 - val_acc: 0.9861\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 18s 73us/step - loss: 0.0400 - acc: 0.9846 - val_loss: 0.0351 - val_acc: 0.9867\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 21s 84us/step - loss: 0.0396 - acc: 0.9850 - val_loss: 0.0420 - val_acc: 0.9854\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 19s 76us/step - loss: 0.0388 - acc: 0.9851 - val_loss: 0.0361 - val_acc: 0.9869\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 22s 88us/step - loss: 0.0389 - acc: 0.9851 - val_loss: 0.0347 - val_acc: 0.9871\n",
      "LEP-0.3-dr-128-l-1-de-1537467876\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 21s 82us/step - loss: 0.1038 - acc: 0.9633 - val_loss: 0.0618 - val_acc: 0.9775\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 20s 81us/step - loss: 0.0592 - acc: 0.9779 - val_loss: 0.0450 - val_acc: 0.9823\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 19s 75us/step - loss: 0.0527 - acc: 0.9805 - val_loss: 0.0480 - val_acc: 0.9830\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 22s 87us/step - loss: 0.0493 - acc: 0.9817 - val_loss: 0.0404 - val_acc: 0.9852\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 20s 80us/step - loss: 0.0482 - acc: 0.9823 - val_loss: 0.0378 - val_acc: 0.9859\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 20s 78us/step - loss: 0.0478 - acc: 0.9825 - val_loss: 0.0375 - val_acc: 0.9859\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 20s 78us/step - loss: 0.0462 - acc: 0.9826 - val_loss: 0.0388 - val_acc: 0.9858\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 20s 81us/step - loss: 0.0456 - acc: 0.9833 - val_loss: 0.0364 - val_acc: 0.9866\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0456 - acc: 0.9833 - val_loss: 0.0365 - val_acc: 0.9865\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 24s 94us/step - loss: 0.0444 - acc: 0.9836 - val_loss: 0.0366 - val_acc: 0.9861\n",
      "LEP-0.5-dr-128-l-1-de-1537468085\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 25s 98us/step - loss: 0.1351 - acc: 0.9529 - val_loss: 0.0577 - val_acc: 0.9791\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 20s 79us/step - loss: 0.0710 - acc: 0.9745 - val_loss: 0.0511 - val_acc: 0.9815\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 20s 80us/step - loss: 0.0642 - acc: 0.9768 - val_loss: 0.0480 - val_acc: 0.9787\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 21s 81us/step - loss: 0.0622 - acc: 0.9777 - val_loss: 0.0434 - val_acc: 0.9831\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 18s 72us/step - loss: 0.0610 - acc: 0.9785 - val_loss: 0.0427 - val_acc: 0.9841\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 18s 69us/step - loss: 0.0585 - acc: 0.9790 - val_loss: 0.0422 - val_acc: 0.9843\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 23s 90us/step - loss: 0.0573 - acc: 0.9794 - val_loss: 0.0422 - val_acc: 0.9844\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 19s 73us/step - loss: 0.0567 - acc: 0.9794 - val_loss: 0.0415 - val_acc: 0.9845\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 21s 82us/step - loss: 0.0556 - acc: 0.9800 - val_loss: 0.0408 - val_acc: 0.9847\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 20s 80us/step - loss: 0.0551 - acc: 0.9800 - val_loss: 0.0396 - val_acc: 0.9859\n",
      "LEP-0.1-dr-32-l-3-de-1537468291\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 18s 72us/step - loss: 0.1440 - acc: 0.9493 - val_loss: 0.0574 - val_acc: 0.9784\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 19s 75us/step - loss: 0.0698 - acc: 0.9751 - val_loss: 0.0465 - val_acc: 0.9831\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 19s 75us/step - loss: 0.0582 - acc: 0.9789 - val_loss: 0.0428 - val_acc: 0.9843\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 17s 69us/step - loss: 0.0533 - acc: 0.9808 - val_loss: 0.0419 - val_acc: 0.9843\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 19s 76us/step - loss: 0.0510 - acc: 0.9812 - val_loss: 0.0407 - val_acc: 0.9843\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 18s 73us/step - loss: 0.0499 - acc: 0.9817 - val_loss: 0.0384 - val_acc: 0.9858\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 19s 74us/step - loss: 0.0479 - acc: 0.9826 - val_loss: 0.0394 - val_acc: 0.9856\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 18s 70us/step - loss: 0.0477 - acc: 0.9825 - val_loss: 0.0400 - val_acc: 0.9852\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 18s 71us/step - loss: 0.0469 - acc: 0.9828 - val_loss: 0.0415 - val_acc: 0.9848\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 18s 69us/step - loss: 0.0467 - acc: 0.9832 - val_loss: 0.0375 - val_acc: 0.9864\n",
      "LEP-0.3-dr-32-l-3-de-1537468475\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 18s 70us/step - loss: 0.2094 - acc: 0.9266 - val_loss: 0.1010 - val_acc: 0.9705\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 18s 72us/step - loss: 0.1003 - acc: 0.9676 - val_loss: 0.0565 - val_acc: 0.9803\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 19s 75us/step - loss: 0.0861 - acc: 0.9722 - val_loss: 0.0558 - val_acc: 0.9786\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 24s 96us/step - loss: 0.0800 - acc: 0.9736 - val_loss: 0.0552 - val_acc: 0.9792\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 18s 71us/step - loss: 0.0765 - acc: 0.9742 - val_loss: 0.0531 - val_acc: 0.9806\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 19s 74us/step - loss: 0.0743 - acc: 0.9753 - val_loss: 0.0499 - val_acc: 0.9813\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 19s 77us/step - loss: 0.0722 - acc: 0.9758 - val_loss: 0.0473 - val_acc: 0.9821\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 20s 80us/step - loss: 0.0700 - acc: 0.9758 - val_loss: 0.0470 - val_acc: 0.9828\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 20s 79us/step - loss: 0.0683 - acc: 0.9765 - val_loss: 0.0491 - val_acc: 0.9783\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 18s 71us/step - loss: 0.0670 - acc: 0.9770 - val_loss: 0.0463 - val_acc: 0.9839\n",
      "LEP-0.5-dr-32-l-3-de-1537468671\n",
      "Train on 251999 samples, validate on 108001 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 18s 71us/step - loss: 0.3258 - acc: 0.8817 - val_loss: 0.1258 - val_acc: 0.9609\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 20s 79us/step - loss: 0.1780 - acc: 0.9508 - val_loss: 0.1103 - val_acc: 0.9661\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 22s 89us/step - loss: 0.1548 - acc: 0.9577 - val_loss: 0.0848 - val_acc: 0.9741\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.1401 - acc: 0.9622 - val_loss: 0.0812 - val_acc: 0.9751\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 17s 69us/step - loss: 0.1335 - acc: 0.9646 - val_loss: 0.0777 - val_acc: 0.9781\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 18s 70us/step - loss: 0.1293 - acc: 0.9651 - val_loss: 0.0764 - val_acc: 0.9768\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 18s 72us/step - loss: 0.1257 - acc: 0.9664 - val_loss: 0.0779 - val_acc: 0.9786\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 18s 71us/step - loss: 0.1254 - acc: 0.9674 - val_loss: 0.0741 - val_acc: 0.9792\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 19s 77us/step - loss: 0.1264 - acc: 0.9674 - val_loss: 0.0754 - val_acc: 0.9776\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 22s 87us/step - loss: 0.1224 - acc: 0.9681 - val_loss: 0.0735 - val_acc: 0.9780\n",
      "LEP-0.1-dr-64-l-3-de-1537468863\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 20s 78us/step - loss: 0.1065 - acc: 0.9625 - val_loss: 0.0571 - val_acc: 0.9783\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 19s 76us/step - loss: 0.0561 - acc: 0.9796 - val_loss: 0.0419 - val_acc: 0.9847\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 19s 76us/step - loss: 0.0505 - acc: 0.9818 - val_loss: 0.0398 - val_acc: 0.9860\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 19s 77us/step - loss: 0.0469 - acc: 0.9829 - val_loss: 0.0400 - val_acc: 0.9853\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 19s 77us/step - loss: 0.0456 - acc: 0.9832 - val_loss: 0.0386 - val_acc: 0.9852\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 19s 75us/step - loss: 0.0441 - acc: 0.9838 - val_loss: 0.0396 - val_acc: 0.9857\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 19s 76us/step - loss: 0.0438 - acc: 0.9840 - val_loss: 0.0377 - val_acc: 0.9855\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0433 - acc: 0.9840 - val_loss: 0.0366 - val_acc: 0.9867\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 21s 82us/step - loss: 0.0422 - acc: 0.9847 - val_loss: 0.0372 - val_acc: 0.9863\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 19s 76us/step - loss: 0.0415 - acc: 0.9846 - val_loss: 0.0372 - val_acc: 0.9867\n",
      "LEP-0.3-dr-64-l-3-de-1537469063\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 20s 80us/step - loss: 0.1487 - acc: 0.9494 - val_loss: 0.0605 - val_acc: 0.9794\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 20s 79us/step - loss: 0.0717 - acc: 0.9751 - val_loss: 0.0469 - val_acc: 0.9824\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 20s 78us/step - loss: 0.0628 - acc: 0.9779 - val_loss: 0.0433 - val_acc: 0.9842\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 20s 80us/step - loss: 0.0596 - acc: 0.9793 - val_loss: 0.0468 - val_acc: 0.9832\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 21s 82us/step - loss: 0.0579 - acc: 0.9793 - val_loss: 0.0454 - val_acc: 0.9843\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 20s 80us/step - loss: 0.0558 - acc: 0.9804 - val_loss: 0.0438 - val_acc: 0.9843\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 21s 83us/step - loss: 0.0563 - acc: 0.9803 - val_loss: 0.0433 - val_acc: 0.9841\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 21s 81us/step - loss: 0.0542 - acc: 0.9809 - val_loss: 0.0397 - val_acc: 0.9852\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 21s 84us/step - loss: 0.0535 - acc: 0.9811 - val_loss: 0.0410 - val_acc: 0.9851\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 20s 78us/step - loss: 0.0533 - acc: 0.9810 - val_loss: 0.0403 - val_acc: 0.9854\n",
      "LEP-0.5-dr-64-l-3-de-1537469269\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 21s 82us/step - loss: 0.2226 - acc: 0.9235 - val_loss: 0.0843 - val_acc: 0.9736\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 20s 80us/step - loss: 0.1047 - acc: 0.9682 - val_loss: 0.0579 - val_acc: 0.9806\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 20s 80us/step - loss: 0.0920 - acc: 0.9712 - val_loss: 0.0559 - val_acc: 0.9800\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 20s 79us/step - loss: 0.0877 - acc: 0.9723 - val_loss: 0.0532 - val_acc: 0.9805\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 21s 84us/step - loss: 0.0847 - acc: 0.9728 - val_loss: 0.0542 - val_acc: 0.9790\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 21s 82us/step - loss: 0.0823 - acc: 0.9734 - val_loss: 0.0517 - val_acc: 0.9804\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 21s 85us/step - loss: 0.0809 - acc: 0.9738 - val_loss: 0.0518 - val_acc: 0.9803\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 21s 82us/step - loss: 0.0799 - acc: 0.9741 - val_loss: 0.0501 - val_acc: 0.9809\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 20s 81us/step - loss: 0.0787 - acc: 0.9746 - val_loss: 0.0515 - val_acc: 0.9806\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 20s 81us/step - loss: 0.0768 - acc: 0.9748 - val_loss: 0.0473 - val_acc: 0.9833\n",
      "LEP-0.1-dr-128-l-3-de-1537469478\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 33s 130us/step - loss: 0.0894 - acc: 0.9685 - val_loss: 0.0590 - val_acc: 0.9802\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 34s 134us/step - loss: 0.0527 - acc: 0.9811 - val_loss: 0.0433 - val_acc: 0.9851\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 34s 133us/step - loss: 0.0483 - acc: 0.9826 - val_loss: 0.0390 - val_acc: 0.9856\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 34s 133us/step - loss: 0.0464 - acc: 0.9832 - val_loss: 0.0417 - val_acc: 0.9845\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 34s 136us/step - loss: 0.0451 - acc: 0.9835 - val_loss: 0.0370 - val_acc: 0.9864\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 32s 128us/step - loss: 0.0438 - acc: 0.9838 - val_loss: 0.0406 - val_acc: 0.9866\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 32s 126us/step - loss: 0.0428 - acc: 0.9842 - val_loss: 0.0355 - val_acc: 0.9865\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 33s 133us/step - loss: 0.0425 - acc: 0.9845 - val_loss: 0.0371 - val_acc: 0.9859\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 32s 127us/step - loss: 0.0423 - acc: 0.9845 - val_loss: 0.0365 - val_acc: 0.9867\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 32s 126us/step - loss: 0.0425 - acc: 0.9847 - val_loss: 0.0362 - val_acc: 0.9870\n",
      "LEP-0.3-dr-128-l-3-de-1537469811\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 34s 134us/step - loss: 0.1109 - acc: 0.9609 - val_loss: 0.0517 - val_acc: 0.9821\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 33s 133us/step - loss: 0.0606 - acc: 0.9781 - val_loss: 0.0423 - val_acc: 0.9840\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 34s 133us/step - loss: 0.0567 - acc: 0.9799 - val_loss: 0.0419 - val_acc: 0.9842\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 33s 131us/step - loss: 0.0545 - acc: 0.9804 - val_loss: 0.0413 - val_acc: 0.9854\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 34s 135us/step - loss: 0.0530 - acc: 0.9811 - val_loss: 0.0435 - val_acc: 0.9834\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 35s 138us/step - loss: 0.0526 - acc: 0.9812 - val_loss: 0.0407 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 32s 127us/step - loss: 0.0522 - acc: 0.9815 - val_loss: 0.0389 - val_acc: 0.9856\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 31s 124us/step - loss: 0.0526 - acc: 0.9819 - val_loss: 0.0404 - val_acc: 0.9846\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 32s 127us/step - loss: 0.0520 - acc: 0.9822 - val_loss: 0.0404 - val_acc: 0.9855\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 32s 127us/step - loss: 0.0523 - acc: 0.9819 - val_loss: 0.0413 - val_acc: 0.9847\n",
      "LEP-0.5-dr-128-l-3-de-1537470145\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 33s 130us/step - loss: 0.1590 - acc: 0.9465 - val_loss: 0.0635 - val_acc: 0.9788\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 30s 119us/step - loss: 0.0814 - acc: 0.9732 - val_loss: 0.0584 - val_acc: 0.9743\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 32s 126us/step - loss: 0.0758 - acc: 0.9746 - val_loss: 0.0500 - val_acc: 0.9814\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 30s 118us/step - loss: 0.0718 - acc: 0.9758 - val_loss: 0.0486 - val_acc: 0.9828\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 29s 114us/step - loss: 0.0698 - acc: 0.9759 - val_loss: 0.0516 - val_acc: 0.9811\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 33s 131us/step - loss: 0.0689 - acc: 0.9768 - val_loss: 0.0496 - val_acc: 0.9823\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 30s 118us/step - loss: 0.0680 - acc: 0.9769 - val_loss: 0.0471 - val_acc: 0.9839\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 30s 118us/step - loss: 0.0681 - acc: 0.9770 - val_loss: 0.0486 - val_acc: 0.9820\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 29s 115us/step - loss: 0.0675 - acc: 0.9773 - val_loss: 0.0467 - val_acc: 0.9828\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 31s 122us/step - loss: 0.0670 - acc: 0.9776 - val_loss: 0.0459 - val_acc: 0.9835\n",
      "LEP-0.1-dr-32-l-5-de-1537470454\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 22s 87us/step - loss: 0.1450 - acc: 0.9492 - val_loss: 0.0587 - val_acc: 0.9804\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 21s 84us/step - loss: 0.0694 - acc: 0.9757 - val_loss: 0.0495 - val_acc: 0.9819\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 21s 83us/step - loss: 0.0598 - acc: 0.9791 - val_loss: 0.0439 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 21s 85us/step - loss: 0.0544 - acc: 0.9805 - val_loss: 0.0457 - val_acc: 0.9830\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 21s 85us/step - loss: 0.0528 - acc: 0.9815 - val_loss: 0.0411 - val_acc: 0.9851\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0508 - acc: 0.9817 - val_loss: 0.0401 - val_acc: 0.9849\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 22s 85us/step - loss: 0.0494 - acc: 0.9821 - val_loss: 0.0395 - val_acc: 0.9858\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 30s 117us/step - loss: 0.0489 - acc: 0.9826 - val_loss: 0.0396 - val_acc: 0.9863\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 27s 109us/step - loss: 0.0484 - acc: 0.9829 - val_loss: 0.0388 - val_acc: 0.9857\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 26s 103us/step - loss: 0.0479 - acc: 0.9828 - val_loss: 0.0382 - val_acc: 0.9862\n",
      "LEP-0.3-dr-32-l-5-de-1537470692\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 26s 102us/step - loss: 0.2414 - acc: 0.9162 - val_loss: 0.0937 - val_acc: 0.9708\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 23s 89us/step - loss: 0.1148 - acc: 0.9678 - val_loss: 0.0737 - val_acc: 0.9774\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 22s 87us/step - loss: 0.0986 - acc: 0.9720 - val_loss: 0.0675 - val_acc: 0.9794\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 22s 85us/step - loss: 0.0935 - acc: 0.9725 - val_loss: 0.0616 - val_acc: 0.9807\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 22s 89us/step - loss: 0.0876 - acc: 0.9734 - val_loss: 0.0525 - val_acc: 0.9823\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 22s 88us/step - loss: 0.0862 - acc: 0.9744 - val_loss: 0.0528 - val_acc: 0.9807\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0821 - acc: 0.9749 - val_loss: 0.0515 - val_acc: 0.9814\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 21s 85us/step - loss: 0.0815 - acc: 0.9749 - val_loss: 0.0541 - val_acc: 0.9822\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0793 - acc: 0.9753 - val_loss: 0.0510 - val_acc: 0.9824\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 22s 87us/step - loss: 0.0793 - acc: 0.9756 - val_loss: 0.0536 - val_acc: 0.9809\n",
      "LEP-0.5-dr-32-l-5-de-1537470920\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 24s 94us/step - loss: 0.4550 - acc: 0.8195 - val_loss: 0.1960 - val_acc: 0.9435\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 23s 89us/step - loss: 0.2636 - acc: 0.9178 - val_loss: 0.1413 - val_acc: 0.9597\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 25s 100us/step - loss: 0.2381 - acc: 0.9319 - val_loss: 0.1444 - val_acc: 0.9637\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 24s 94us/step - loss: 0.2209 - acc: 0.9377 - val_loss: 0.1299 - val_acc: 0.9644\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 24s 93us/step - loss: 0.2044 - acc: 0.9428 - val_loss: 0.1262 - val_acc: 0.9655\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 24s 94us/step - loss: 0.1985 - acc: 0.9452 - val_loss: 0.1229 - val_acc: 0.9620\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 23s 90us/step - loss: 0.1909 - acc: 0.9464 - val_loss: 0.1115 - val_acc: 0.9692\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 24s 95us/step - loss: 0.1854 - acc: 0.9491 - val_loss: 0.1013 - val_acc: 0.9721\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 24s 95us/step - loss: 0.1789 - acc: 0.9514 - val_loss: 0.1061 - val_acc: 0.9746\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 25s 98us/step - loss: 0.1759 - acc: 0.9536 - val_loss: 0.0977 - val_acc: 0.9770\n",
      "LEP-0.1-dr-64-l-5-de-1537471163\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 35s 139us/step - loss: 0.1063 - acc: 0.9637 - val_loss: 0.0488 - val_acc: 0.9810\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 27s 109us/step - loss: 0.0574 - acc: 0.9792 - val_loss: 0.0537 - val_acc: 0.9820\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 27s 108us/step - loss: 0.0529 - acc: 0.9806 - val_loss: 0.0438 - val_acc: 0.9844\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 27s 106us/step - loss: 0.0499 - acc: 0.9817 - val_loss: 0.0435 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 28s 112us/step - loss: 0.0488 - acc: 0.9824 - val_loss: 0.0384 - val_acc: 0.9862\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 27s 108us/step - loss: 0.0478 - acc: 0.9828 - val_loss: 0.0401 - val_acc: 0.9852\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 27s 107us/step - loss: 0.0471 - acc: 0.9832 - val_loss: 0.0391 - val_acc: 0.9855\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 27s 106us/step - loss: 0.0460 - acc: 0.9833 - val_loss: 0.0390 - val_acc: 0.9855\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 28s 112us/step - loss: 0.0468 - acc: 0.9834 - val_loss: 0.0382 - val_acc: 0.9857\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 28s 111us/step - loss: 0.0458 - acc: 0.9834 - val_loss: 0.0400 - val_acc: 0.9858\n",
      "LEP-0.3-dr-64-l-5-de-1537471451\n",
      "Train on 251999 samples, validate on 108001 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 35s 140us/step - loss: 0.1587 - acc: 0.9459 - val_loss: 0.0713 - val_acc: 0.9739\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 31s 123us/step - loss: 0.0785 - acc: 0.9744 - val_loss: 0.0504 - val_acc: 0.9816\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 32s 128us/step - loss: 0.0700 - acc: 0.9766 - val_loss: 0.0501 - val_acc: 0.9804\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 32s 128us/step - loss: 0.0672 - acc: 0.9770 - val_loss: 0.0491 - val_acc: 0.9814\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 30s 120us/step - loss: 0.0648 - acc: 0.9776 - val_loss: 0.0468 - val_acc: 0.9826\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 27s 109us/step - loss: 0.0645 - acc: 0.9775 - val_loss: 0.0463 - val_acc: 0.9832\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 29s 113us/step - loss: 0.0627 - acc: 0.9787 - val_loss: 0.0471 - val_acc: 0.9832\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 27s 108us/step - loss: 0.0622 - acc: 0.9789 - val_loss: 0.0466 - val_acc: 0.9837\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 28s 112us/step - loss: 0.0617 - acc: 0.9790 - val_loss: 0.0466 - val_acc: 0.9826\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 28s 112us/step - loss: 0.0609 - acc: 0.9791 - val_loss: 0.0451 - val_acc: 0.9825\n",
      "LEP-0.5-dr-64-l-5-de-1537471758\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 28s 112us/step - loss: 0.3096 - acc: 0.8880 - val_loss: 0.1399 - val_acc: 0.9614\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 28s 112us/step - loss: 0.1583 - acc: 0.9562 - val_loss: 0.0873 - val_acc: 0.9747\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 28s 110us/step - loss: 0.1334 - acc: 0.9630 - val_loss: 0.0835 - val_acc: 0.9707\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 28s 111us/step - loss: 0.1209 - acc: 0.9662 - val_loss: 0.0690 - val_acc: 0.9730\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 29s 117us/step - loss: 0.1125 - acc: 0.9683 - val_loss: 0.0690 - val_acc: 0.9733\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 27s 106us/step - loss: 0.1088 - acc: 0.9692 - val_loss: 0.0677 - val_acc: 0.9770\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 27s 108us/step - loss: 0.1064 - acc: 0.9693 - val_loss: 0.0659 - val_acc: 0.9734\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 27s 107us/step - loss: 0.1079 - acc: 0.9691 - val_loss: 0.0598 - val_acc: 0.9778\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 29s 114us/step - loss: 0.1035 - acc: 0.9704 - val_loss: 0.0692 - val_acc: 0.9702\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 28s 112us/step - loss: 0.1016 - acc: 0.9707 - val_loss: 0.0690 - val_acc: 0.9710\n",
      "LEP-0.1-dr-128-l-5-de-1537472043\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 44s 176us/step - loss: 0.0972 - acc: 0.9660 - val_loss: 0.0508 - val_acc: 0.9814\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 42s 168us/step - loss: 0.0559 - acc: 0.9800 - val_loss: 0.0476 - val_acc: 0.9821\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 49s 193us/step - loss: 0.0509 - acc: 0.9818 - val_loss: 0.0411 - val_acc: 0.9848\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 42s 166us/step - loss: 0.0495 - acc: 0.9827 - val_loss: 0.0418 - val_acc: 0.9854\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 50s 199us/step - loss: 0.0485 - acc: 0.9829 - val_loss: 0.0399 - val_acc: 0.9856\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 53s 210us/step - loss: 0.0479 - acc: 0.9832 - val_loss: 0.0583 - val_acc: 0.9778\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 49s 195us/step - loss: 0.0480 - acc: 0.9834 - val_loss: 0.0397 - val_acc: 0.9860\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 55s 220us/step - loss: 0.0485 - acc: 0.9833 - val_loss: 0.0424 - val_acc: 0.9842\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 51s 204us/step - loss: 0.0474 - acc: 0.9837 - val_loss: 0.0400 - val_acc: 0.9855\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 59s 233us/step - loss: 0.0464 - acc: 0.9836 - val_loss: 0.0399 - val_acc: 0.9858\n",
      "LEP-0.3-dr-128-l-5-de-1537472544\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 48s 189us/step - loss: 0.1236 - acc: 0.9581 - val_loss: 0.0578 - val_acc: 0.9765\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 49s 193us/step - loss: 0.0698 - acc: 0.9761 - val_loss: 0.0491 - val_acc: 0.9811\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 55s 216us/step - loss: 0.0648 - acc: 0.9772 - val_loss: 0.0565 - val_acc: 0.9811\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 51s 201us/step - loss: 0.0619 - acc: 0.9782 - val_loss: 0.0457 - val_acc: 0.9833\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 52s 207us/step - loss: 0.0618 - acc: 0.9785 - val_loss: 0.0485 - val_acc: 0.9825\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 49s 194us/step - loss: 0.0607 - acc: 0.9792 - val_loss: 0.0490 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 50s 197us/step - loss: 0.0595 - acc: 0.9795 - val_loss: 0.0437 - val_acc: 0.9851\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 51s 202us/step - loss: 0.0611 - acc: 0.9793 - val_loss: 0.0516 - val_acc: 0.9838\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 50s 198us/step - loss: 0.0630 - acc: 0.9791 - val_loss: 0.0468 - val_acc: 0.9836\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 50s 198us/step - loss: 0.0616 - acc: 0.9799 - val_loss: 0.0472 - val_acc: 0.9826\n",
      "LEP-0.5-dr-128-l-5-de-1537473055\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 54s 214us/step - loss: 0.1869 - acc: 0.9371 - val_loss: 0.0751 - val_acc: 0.9774\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 49s 196us/step - loss: 0.0990 - acc: 0.9701 - val_loss: 0.0631 - val_acc: 0.9789\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 51s 203us/step - loss: 0.0909 - acc: 0.9725 - val_loss: 0.0690 - val_acc: 0.9777\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 53s 210us/step - loss: 0.0867 - acc: 0.9732 - val_loss: 0.0565 - val_acc: 0.9803\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 51s 204us/step - loss: 0.0871 - acc: 0.9736 - val_loss: 0.0575 - val_acc: 0.9782\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 51s 203us/step - loss: 0.0843 - acc: 0.9737 - val_loss: 0.0565 - val_acc: 0.9785\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 52s 207us/step - loss: 0.0835 - acc: 0.9739 - val_loss: 0.0560 - val_acc: 0.9749\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 52s 207us/step - loss: 0.0839 - acc: 0.9739 - val_loss: 0.0575 - val_acc: 0.9813\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 52s 205us/step - loss: 0.0834 - acc: 0.9747 - val_loss: 0.0551 - val_acc: 0.9811\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 51s 204us/step - loss: 0.0839 - acc: 0.9743 - val_loss: 0.0606 - val_acc: 0.9806\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [1,3,5]\n",
    "layer_sizes = [32,64,128]\n",
    "drop_layers = [0.1,0.3,0.5]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for drop_layer in drop_layers:\n",
    "            NAME = \"LEP-{}-dr-{}-l-{}-de-{}\".format(drop_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)\n",
    "            tensorboard = TensorBoard(log_dir = 'log/{}'.format(NAME))\n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(Dense(layer_size,input_shape = (14,)))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Dropout(drop_layer))                                       # dropout 10% of the neurons\n",
    "#           model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#           model.add(Flatten())\n",
    "            for l in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(Dropout(drop_layer))# dropout 10% of the neurons\n",
    "            \n",
    "            model.add(Dense(4))\n",
    "            model.add(Activation('softmax'))\n",
    "            \n",
    "##qual otimizador?\n",
    "            model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "            model.fit(X_train, y_train,\n",
    "                      batch_size=32,\n",
    "                      epochs=10,\n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[tensorboard])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best models are:\n",
    "## LEP-0.1-dr-128-l-1-de-1537063651\n",
    "## LEP-0.1-dr-128-l-3-de-1537469478\n",
    "## LEP-0.3-dr-128-l-1-de-1537467876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best-LEP-0.3-dr-128-l-1-de\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/30\n",
      "251999/251999 [==============================] - 31s 123us/step - loss: 0.1047 - acc: 0.9632 - val_loss: 0.0561 - val_acc: 0.9800\n",
      "Epoch 2/30\n",
      "251999/251999 [==============================] - 29s 115us/step - loss: 0.0594 - acc: 0.9780 - val_loss: 0.0461 - val_acc: 0.9831\n",
      "Epoch 3/30\n",
      "251999/251999 [==============================] - 33s 130us/step - loss: 0.0530 - acc: 0.9806 - val_loss: 0.0413 - val_acc: 0.9839\n",
      "Epoch 4/30\n",
      "251999/251999 [==============================] - 32s 128us/step - loss: 0.0502 - acc: 0.9816 - val_loss: 0.0379 - val_acc: 0.9855\n",
      "Epoch 5/30\n",
      "251999/251999 [==============================] - 29s 116us/step - loss: 0.0490 - acc: 0.9820 - val_loss: 0.0373 - val_acc: 0.9857\n",
      "Epoch 6/30\n",
      "251999/251999 [==============================] - 31s 124us/step - loss: 0.0472 - acc: 0.9824 - val_loss: 0.0369 - val_acc: 0.9861\n",
      "Epoch 7/30\n",
      "251999/251999 [==============================] - 32s 128us/step - loss: 0.0467 - acc: 0.9827 - val_loss: 0.0377 - val_acc: 0.9863\n",
      "Epoch 8/30\n",
      "251999/251999 [==============================] - 30s 121us/step - loss: 0.0460 - acc: 0.9831 - val_loss: 0.0377 - val_acc: 0.9862\n",
      "Epoch 9/30\n",
      "251999/251999 [==============================] - 28s 112us/step - loss: 0.0448 - acc: 0.9832 - val_loss: 0.0367 - val_acc: 0.9859\n",
      "Epoch 10/30\n",
      "251999/251999 [==============================] - 29s 115us/step - loss: 0.0451 - acc: 0.9831 - val_loss: 0.0366 - val_acc: 0.9862\n",
      "Epoch 11/30\n",
      "251999/251999 [==============================] - 33s 131us/step - loss: 0.0444 - acc: 0.9836 - val_loss: 0.0354 - val_acc: 0.9868\n",
      "Epoch 12/30\n",
      "251999/251999 [==============================] - 29s 117us/step - loss: 0.0440 - acc: 0.9836 - val_loss: 0.0364 - val_acc: 0.9855\n",
      "Epoch 13/30\n",
      "251999/251999 [==============================] - 28s 112us/step - loss: 0.0434 - acc: 0.9839 - val_loss: 0.0348 - val_acc: 0.9870\n",
      "Epoch 14/30\n",
      "251999/251999 [==============================] - 29s 117us/step - loss: 0.0438 - acc: 0.9841 - val_loss: 0.0364 - val_acc: 0.9857\n",
      "Epoch 15/30\n",
      "251999/251999 [==============================] - 32s 127us/step - loss: 0.0433 - acc: 0.9841 - val_loss: 0.0360 - val_acc: 0.9870\n",
      "Epoch 16/30\n",
      "251999/251999 [==============================] - 31s 123us/step - loss: 0.0430 - acc: 0.9842 - val_loss: 0.0357 - val_acc: 0.9870\n",
      "Epoch 17/30\n",
      "251999/251999 [==============================] - 33s 129us/step - loss: 0.0432 - acc: 0.9837 - val_loss: 0.0375 - val_acc: 0.9862\n",
      "Epoch 18/30\n",
      "251999/251999 [==============================] - 31s 123us/step - loss: 0.0428 - acc: 0.9841 - val_loss: 0.0353 - val_acc: 0.9867\n",
      "Epoch 19/30\n",
      "251999/251999 [==============================] - 32s 128us/step - loss: 0.0427 - acc: 0.9843 - val_loss: 0.0359 - val_acc: 0.9865\n",
      "Epoch 20/30\n",
      "251999/251999 [==============================] - 35s 138us/step - loss: 0.0426 - acc: 0.9845 - val_loss: 0.0358 - val_acc: 0.9863\n",
      "Epoch 21/30\n",
      "251999/251999 [==============================] - 34s 134us/step - loss: 0.0426 - acc: 0.9842 - val_loss: 0.0377 - val_acc: 0.9862\n",
      "Epoch 22/30\n",
      "251999/251999 [==============================] - 31s 125us/step - loss: 0.0422 - acc: 0.9845 - val_loss: 0.0341 - val_acc: 0.9875\n",
      "Epoch 23/30\n",
      "251999/251999 [==============================] - 30s 119us/step - loss: 0.0423 - acc: 0.9846 - val_loss: 0.0349 - val_acc: 0.9871\n",
      "Epoch 24/30\n",
      "251999/251999 [==============================] - 29s 114us/step - loss: 0.0424 - acc: 0.9845 - val_loss: 0.0374 - val_acc: 0.9871\n",
      "Epoch 25/30\n",
      "251999/251999 [==============================] - 35s 138us/step - loss: 0.0423 - acc: 0.9843 - val_loss: 0.0374 - val_acc: 0.9863\n",
      "Epoch 26/30\n",
      "251999/251999 [==============================] - 31s 123us/step - loss: 0.0423 - acc: 0.9846 - val_loss: 0.0344 - val_acc: 0.9864\n",
      "Epoch 27/30\n",
      "251999/251999 [==============================] - 32s 128us/step - loss: 0.0417 - acc: 0.9846 - val_loss: 0.0356 - val_acc: 0.9871\n",
      "Epoch 28/30\n",
      "251999/251999 [==============================] - 30s 118us/step - loss: 0.0422 - acc: 0.9846 - val_loss: 0.0355 - val_acc: 0.9871\n",
      "Epoch 29/30\n",
      "251999/251999 [==============================] - 34s 133us/step - loss: 0.0420 - acc: 0.9847 - val_loss: 0.0342 - val_acc: 0.9877\n",
      "Epoch 30/30\n",
      "251999/251999 [==============================] - 35s 137us/step - loss: 0.0418 - acc: 0.9848 - val_loss: 0.0352 - val_acc: 0.9873\n"
     ]
    }
   ],
   "source": [
    "dense_layer = 1\n",
    "layer_size = 128\n",
    "drop_layer = 0.3\n",
    "\n",
    "\n",
    "NAME1 = \"best-LEP-{}-dr-{}-l-{}-de\".format(drop_layer, layer_size, dense_layer)\n",
    "print(NAME1)\n",
    "tensorboard = TensorBoard(log_dir = 'log/{}'.format(NAME1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(layer_size,input_shape = (int(14),)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(drop_layer))                                       # dropout 10% of the neurons\n",
    "#           model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#           model.add(Flatten())\n",
    "for l in range(dense_layer):\n",
    "    model.add(Dense(layer_size))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop_layer))# dropout 10% of the neurons\n",
    "\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir = 'log/{}'.format(NAME_final))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "LEP_model = model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=30,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 2s 51us/step\n",
      "Loss: 0.035254014251986515\n",
      "Accuracy: 0.986725\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test,y_test)\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best-LEP-0.3-dr-128-l-1-de.model'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomemodelo = str(NAME1)+'.model'\n",
    "nomemodelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final-LEP-0.3-dr-128-l-1-de.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final-LEP-0.3-dr-128-l-1-de.model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUEopBWggKKWUcmggKKWUAuD/A5e9NVPGf2E1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1291a0866a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import tensorboard data\n",
    "tb = pd.read_csv('run_best-LEP-0.3-dr-128-l-1-de-tag-acc.csv',sep=',')\n",
    "tba = pd.read_csv('run_best-LEP-0.3-dr-128-l-1-de-tag-val_acc.csv',sep=',')\n",
    "\n",
    "print('final-LEP-0.3-dr-128-l-1-de.model')\n",
    "\n",
    "plt.plot(tb['Value'],label = 'acc')\n",
    "plt.plot(tba['Value'],label = 'val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEP_new_model = tf.keras.models.load_model('final-LEP-0.3-dr-128-l-1-de.model') #loading the model\n",
    "\n",
    "predictions = LEP_new_model.predict([X_test]) #testing the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "itt= random.randint(0,len(y_test))\n",
    "itt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: Muon\n"
     ]
    }
   ],
   "source": [
    "print('Event:', CATEGORIES[y_test[itt]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Muon\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Prediction:',CATEGORIES[np.argmax((predictions[itt]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
