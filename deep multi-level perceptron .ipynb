{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names:  Yuri MÃ¼ller Plumm (CBPF)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Monte Carlo Simulation\n",
    "df_ee = pd.read_hdf('z0_mc_ee.h5')\n",
    "df_mm = pd.read_hdf('z0_mc_mm.h5')\n",
    "df_tt = pd.read_hdf('z0_mc_tt.h5')\n",
    "df_qq = pd.read_hdf('z0_mc_qq.h5')\n",
    "\n",
    "#Data\n",
    "df_data1 = pd.read_hdf('z0_data1.h5')\n",
    "df_data2 = pd.read_hdf('z0_data2.h5')\n",
    "df_data3 = pd.read_hdf('z0_data3.h5')\n",
    "df_data4 = pd.read_hdf('z0_data4.h5')\n",
    "df_data5 = pd.read_hdf('z0_data5.h5')\n",
    "df_data6 = pd.read_hdf('z0_data6.h5')\n",
    "df_data7 = pd.read_hdf('z0_data7.h5')\n",
    "\n",
    "#Defining each channel, using the monte carlo of each channel\n",
    "lista_0 = [0 for i in range(100000)]\n",
    "class_ee = pd.DataFrame({'class':lista_0})\n",
    "lista_1 = [1 for i in range(100000)]\n",
    "class_mm = pd.DataFrame({'class':lista_1})\n",
    "lista_2 = [2 for i in range(100000)]\n",
    "class_tt = pd.DataFrame({'class':lista_2})\n",
    "lista_3 = [3 for i in range(100000)]\n",
    "class_qq = pd.DataFrame({'class':lista_3})\n",
    "\n",
    "df_ee_final = df_ee.join(class_ee)\n",
    "df_mm_final = df_mm.join(class_mm)\n",
    "df_tt_final = df_tt.join(class_tt)\n",
    "df_qq_final = df_qq.join(class_qq)\n",
    "\n",
    "#Taking off some non-prhisical feature of the data that can confuse the NN\n",
    "result1 = pd.concat([df_ee_final,df_mm_final,df_tt_final,df_qq_final])\n",
    "result0 = result1.drop(['run','event','e_lep'],axis=1)\n",
    "#result.head()\n",
    "\n",
    "df_data1_neww = df_data1.drop(['run','event','e_lep'],axis=1)\n",
    "df_data2_neww = df_data2.drop(['run','event','e_lep'],axis=1)\n",
    "df_data3_neww = df_data3.drop(['run','event','e_lep'],axis=1)\n",
    "df_data4_neww = df_data4.drop(['run','event','e_lep'],axis=1)\n",
    "df_data5_neww = df_data5.drop(['run','event','e_lep'],axis=1)\n",
    "df_data6_neww = df_data6.drop(['run','event','e_lep'],axis=1)\n",
    "df_data7_neww = df_data7.drop(['run','event','e_lep'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = ['Electron','Muon','Tau','Quark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acol</th>\n",
       "      <th>cos_thrust</th>\n",
       "      <th>cos_thrust_neg</th>\n",
       "      <th>cos_thrust_pos</th>\n",
       "      <th>d0_mean</th>\n",
       "      <th>e_ecal</th>\n",
       "      <th>e_hcal</th>\n",
       "      <th>n_charged</th>\n",
       "      <th>n_ecal</th>\n",
       "      <th>n_muons</th>\n",
       "      <th>p_charged</th>\n",
       "      <th>phi_thrust</th>\n",
       "      <th>thrust</th>\n",
       "      <th>z0_mean</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.478470</td>\n",
       "      <td>-0.857441</td>\n",
       "      <td>-0.860064</td>\n",
       "      <td>0.855789</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>88.929619</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>81.328445</td>\n",
       "      <td>-118.945854</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>-0.114538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.073654</td>\n",
       "      <td>-0.360697</td>\n",
       "      <td>-0.361289</td>\n",
       "      <td>0.360372</td>\n",
       "      <td>0.026090</td>\n",
       "      <td>90.303406</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>71.335449</td>\n",
       "      <td>74.006271</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.052427</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.383408</td>\n",
       "      <td>-0.971304</td>\n",
       "      <td>-0.970553</td>\n",
       "      <td>0.971979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.632736</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.534874</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.977844</td>\n",
       "      <td>0.420864</td>\n",
       "      <td>0.468641</td>\n",
       "      <td>-0.395415</td>\n",
       "      <td>0.023564</td>\n",
       "      <td>90.568001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>62.687538</td>\n",
       "      <td>35.988522</td>\n",
       "      <td>0.963187</td>\n",
       "      <td>0.694042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.137634</td>\n",
       "      <td>-0.968132</td>\n",
       "      <td>-0.968279</td>\n",
       "      <td>0.967988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.347679</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-155.903839</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acol  cos_thrust  cos_thrust_neg  cos_thrust_pos   d0_mean     e_ecal  \\\n",
       "0  0.478470   -0.857441       -0.860064        0.855789  0.012637  88.929619   \n",
       "1  0.073654   -0.360697       -0.361289        0.360372  0.026090  90.303406   \n",
       "2  0.383408   -0.971304       -0.970553        0.971979  0.000000  89.632736   \n",
       "3  8.977844    0.420864        0.468641       -0.395415  0.023564  90.568001   \n",
       "4  0.137634   -0.968132       -0.968279        0.967988  0.000000  84.347679   \n",
       "\n",
       "   e_hcal  n_charged  n_ecal  n_muons  p_charged  phi_thrust    thrust  \\\n",
       "0    0.00          2       2        0  81.328445 -118.945854  0.999992   \n",
       "1    0.00          2       2        0  71.335449   74.006271  1.000000   \n",
       "2    3.19          0       2        0   0.000000 -100.534874  0.999994   \n",
       "3    0.00          2       3        0  62.687538   35.988522  0.963187   \n",
       "4    2.10          0       2        0   0.000000 -155.903839  0.999999   \n",
       "\n",
       "    z0_mean  class  \n",
       "0 -0.114538      0  \n",
       "1 -0.052427      0  \n",
       "2  0.000000      0  \n",
       "3  0.694042      0  \n",
       "4  0.000000      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result0.sample(frac=1).reset_index(drop=True)  #shuffle the rows only\n",
    "#Xclass = result.drop('class',axis=0)\n",
    "#yclass = result['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xclass = result.drop('class',axis=1)\n",
    "yclass = result['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w8FWxMTyeS6EgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bc5eabf828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yclass.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = np.array(Xclass)\n",
    "y_array = np.array(yclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(y_array)#total number of simulations\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "def creating_training_data():\n",
    "    for i in range(int(N)):  # do dogs and cats\n",
    "        try:\n",
    "            event = X_array[i]  # convert to array\n",
    "            tipo = y_array[i]\n",
    "            training_data.append([event,tipo])\n",
    "        except Exception as e:   #I should put some warning here\n",
    "                pass\n",
    "\n",
    "creating_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#random.shuffle(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.14367676e+00,  9.29805219e-01,  9.29267645e-01, -9.30635810e-01,\n",
       "         1.09610632e-02,  9.37013702e+01,  0.00000000e+00,  3.00000000e+00,\n",
       "         3.00000000e+00,  0.00000000e+00,  8.81172657e+00,  7.22930527e+01,\n",
       "         9.99872506e-01,  1.82751548e+00]), 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for imgs,types in training_data:\n",
    "    \n",
    "    y.append(types)\n",
    "    X.append(imgs)\n",
    "\n",
    "print(np.size(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"XLEP.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"yLEP.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data saved. Now we could construct our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential  # for a sequential model \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import os\n",
    "#import cv2\n",
    "#from tqdm import tqdm\n",
    "#import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_in = open(\"XLEP.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"yLEP.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "X = np.array(X) #normalizing data\n",
    "\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 10\n",
    "X_test = np.array([X[i] for i in range((len(X)//df))])\n",
    "X_train = np.array([X[i] for i in range((len(X)//df),len(X))])\n",
    "\n",
    "y_test = np.array([y[i] for i in range((len(y)//df))])\n",
    "y_train = np.array([y[i] for i in range((len(y)//df),len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.11813302,  0.10480576,  0.13712253,  0.12398653, -0.50759472,\n",
       "         1.54449392, -0.85267234, -0.42099135, -0.5089397 , -0.61721897,\n",
       "        -0.02966687, -0.0358371 ,  0.096172  ,  0.72876074]), 14)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0],len(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refazer a anÃ¡lise abaixo com os dados tratados corretamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEP0.1-dr-32-l-1-de-1537062896\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 10s 41us/step - loss: 0.1270 - acc: 0.9552 - val_loss: 0.0601 - val_acc: 0.9781\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 10s 39us/step - loss: 0.0639 - acc: 0.9770 - val_loss: 0.0540 - val_acc: 0.9807\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 10s 39us/step - loss: 0.0546 - acc: 0.9798 - val_loss: 0.0498 - val_acc: 0.9801\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 10s 40us/step - loss: 0.0503 - acc: 0.9815 - val_loss: 0.0428 - val_acc: 0.9832\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 10s 41us/step - loss: 0.0472 - acc: 0.9825 - val_loss: 0.0415 - val_acc: 0.9842\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 10s 40us/step - loss: 0.0460 - acc: 0.9828 - val_loss: 0.0433 - val_acc: 0.9835\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 10s 39us/step - loss: 0.0446 - acc: 0.9834 - val_loss: 0.0412 - val_acc: 0.9836\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 10s 39us/step - loss: 0.0438 - acc: 0.9838 - val_loss: 0.0402 - val_acc: 0.9844\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 10s 39us/step - loss: 0.0435 - acc: 0.9837 - val_loss: 0.0372 - val_acc: 0.9856\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 12s 47us/step - loss: 0.0426 - acc: 0.9842 - val_loss: 0.0375 - val_acc: 0.9856\n",
      "LEP0.3-dr-32-l-1-de-1537063000\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 18s 70us/step - loss: 0.1798 - acc: 0.9352 - val_loss: 0.0713 - val_acc: 0.9763\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 14s 55us/step - loss: 0.0870 - acc: 0.9710 - val_loss: 0.0587 - val_acc: 0.9779\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 12s 49us/step - loss: 0.0760 - acc: 0.9740 - val_loss: 0.0517 - val_acc: 0.9801\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 12s 48us/step - loss: 0.0701 - acc: 0.9759 - val_loss: 0.0520 - val_acc: 0.9805\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 13s 50us/step - loss: 0.0684 - acc: 0.9762 - val_loss: 0.0507 - val_acc: 0.9798\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 15s 61us/step - loss: 0.0657 - acc: 0.9766 - val_loss: 0.0481 - val_acc: 0.9820\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 13s 52us/step - loss: 0.0650 - acc: 0.9770 - val_loss: 0.0493 - val_acc: 0.9823\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 16s 63us/step - loss: 0.0625 - acc: 0.9774 - val_loss: 0.0461 - val_acc: 0.9825\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 12s 46us/step - loss: 0.0625 - acc: 0.9780 - val_loss: 0.0462 - val_acc: 0.9824\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 12s 48us/step - loss: 0.0615 - acc: 0.9783 - val_loss: 0.0448 - val_acc: 0.9830\n",
      "LEP0.5-dr-32-l-1-de-1537063138\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 11s 44us/step - loss: 0.2568 - acc: 0.9087 - val_loss: 0.0924 - val_acc: 0.9705\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 10s 41us/step - loss: 0.1359 - acc: 0.9554 - val_loss: 0.0708 - val_acc: 0.9751\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 10s 41us/step - loss: 0.1177 - acc: 0.9611 - val_loss: 0.0665 - val_acc: 0.9769\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 10s 41us/step - loss: 0.1100 - acc: 0.9640 - val_loss: 0.0605 - val_acc: 0.9781\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 10s 40us/step - loss: 0.1037 - acc: 0.9654 - val_loss: 0.0613 - val_acc: 0.9797\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 13s 51us/step - loss: 0.1025 - acc: 0.9656 - val_loss: 0.0616 - val_acc: 0.9784\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 10s 41us/step - loss: 0.1001 - acc: 0.9669 - val_loss: 0.0577 - val_acc: 0.9797\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 10s 41us/step - loss: 0.0977 - acc: 0.9674 - val_loss: 0.0591 - val_acc: 0.9791\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 11s 45us/step - loss: 0.0994 - acc: 0.9676 - val_loss: 0.0590 - val_acc: 0.9795\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 11s 45us/step - loss: 0.0966 - acc: 0.9680 - val_loss: 0.0599 - val_acc: 0.9793\n",
      "LEP0.1-dr-64-l-1-de-1537063247\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 12s 48us/step - loss: 0.1028 - acc: 0.9636 - val_loss: 0.0575 - val_acc: 0.9783\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 12s 46us/step - loss: 0.0542 - acc: 0.9799 - val_loss: 0.0474 - val_acc: 0.9811\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 12s 46us/step - loss: 0.0484 - acc: 0.9821 - val_loss: 0.0434 - val_acc: 0.9838\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 12s 46us/step - loss: 0.0447 - acc: 0.9831 - val_loss: 0.0411 - val_acc: 0.9848\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 12s 46us/step - loss: 0.0431 - acc: 0.9837 - val_loss: 0.0385 - val_acc: 0.9856\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 12s 46us/step - loss: 0.0421 - acc: 0.9840 - val_loss: 0.0386 - val_acc: 0.9848\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 12s 47us/step - loss: 0.0414 - acc: 0.9846 - val_loss: 0.0385 - val_acc: 0.9844\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 13s 52us/step - loss: 0.0408 - acc: 0.9848 - val_loss: 0.0369 - val_acc: 0.9857\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 12s 49us/step - loss: 0.0397 - acc: 0.9849 - val_loss: 0.0359 - val_acc: 0.9863\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 12s 47us/step - loss: 0.0390 - acc: 0.9852 - val_loss: 0.0400 - val_acc: 0.9847\n",
      "LEP0.3-dr-64-l-1-de-1537063367\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 13s 50us/step - loss: 0.1282 - acc: 0.9549 - val_loss: 0.0609 - val_acc: 0.9766\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 13s 51us/step - loss: 0.0674 - acc: 0.9762 - val_loss: 0.0504 - val_acc: 0.9814\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 12s 49us/step - loss: 0.0583 - acc: 0.9787 - val_loss: 0.0515 - val_acc: 0.9803\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 12s 48us/step - loss: 0.0553 - acc: 0.9804 - val_loss: 0.0456 - val_acc: 0.9822\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 13s 51us/step - loss: 0.0518 - acc: 0.9808 - val_loss: 0.0429 - val_acc: 0.9835\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 12s 49us/step - loss: 0.0513 - acc: 0.9817 - val_loss: 0.0405 - val_acc: 0.9846\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 12s 49us/step - loss: 0.0498 - acc: 0.9815 - val_loss: 0.0414 - val_acc: 0.9842\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 12s 49us/step - loss: 0.0491 - acc: 0.9820 - val_loss: 0.0393 - val_acc: 0.9847\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 12s 48us/step - loss: 0.0483 - acc: 0.9823 - val_loss: 0.0398 - val_acc: 0.9847\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 12s 49us/step - loss: 0.0475 - acc: 0.9825 - val_loss: 0.0399 - val_acc: 0.9843\n",
      "LEP0.5-dr-64-l-1-de-1537063493\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 13s 52us/step - loss: 0.1741 - acc: 0.9396 - val_loss: 0.0708 - val_acc: 0.9751\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 13s 51us/step - loss: 0.0840 - acc: 0.9721 - val_loss: 0.0551 - val_acc: 0.9799\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 18s 70us/step - loss: 0.0734 - acc: 0.9751 - val_loss: 0.0512 - val_acc: 0.9812\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 17s 69us/step - loss: 0.0691 - acc: 0.9765 - val_loss: 0.0487 - val_acc: 0.9819\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 15s 59us/step - loss: 0.0666 - acc: 0.9772 - val_loss: 0.0477 - val_acc: 0.9815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 13s 53us/step - loss: 0.0663 - acc: 0.9771 - val_loss: 0.0477 - val_acc: 0.9823\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 16s 63us/step - loss: 0.0642 - acc: 0.9776 - val_loss: 0.0458 - val_acc: 0.9830\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 18s 72us/step - loss: 0.0631 - acc: 0.9782 - val_loss: 0.0466 - val_acc: 0.9823\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 17s 68us/step - loss: 0.0617 - acc: 0.9785 - val_loss: 0.0468 - val_acc: 0.9829\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 15s 61us/step - loss: 0.0627 - acc: 0.9781 - val_loss: 0.0445 - val_acc: 0.9838\n",
      "LEP0.1-dr-128-l-1-de-1537063651\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 17s 68us/step - loss: 0.0889 - acc: 0.9684 - val_loss: 0.0518 - val_acc: 0.9809\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0500 - acc: 0.9813 - val_loss: 0.0428 - val_acc: 0.9830\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 18s 71us/step - loss: 0.0447 - acc: 0.9829 - val_loss: 0.0410 - val_acc: 0.9846\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 19s 74us/step - loss: 0.0427 - acc: 0.9838 - val_loss: 0.0410 - val_acc: 0.9850\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 21s 83us/step - loss: 0.0412 - acc: 0.9845 - val_loss: 0.0504 - val_acc: 0.9815\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 18s 73us/step - loss: 0.0398 - acc: 0.9848 - val_loss: 0.0406 - val_acc: 0.9851\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 18s 73us/step - loss: 0.0391 - acc: 0.9852 - val_loss: 0.0397 - val_acc: 0.9850\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 16s 63us/step - loss: 0.0384 - acc: 0.9856 - val_loss: 0.0376 - val_acc: 0.9858\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 16s 62us/step - loss: 0.0379 - acc: 0.9859 - val_loss: 0.0360 - val_acc: 0.9861\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 16s 62us/step - loss: 0.0377 - acc: 0.9859 - val_loss: 0.0359 - val_acc: 0.9861\n",
      "LEP0.3-dr-128-l-1-de-1537063827\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 18s 72us/step - loss: 0.1065 - acc: 0.9625 - val_loss: 0.0542 - val_acc: 0.9788\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 18s 70us/step - loss: 0.0589 - acc: 0.9785 - val_loss: 0.0468 - val_acc: 0.9825\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 18s 72us/step - loss: 0.0520 - acc: 0.9806 - val_loss: 0.0432 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 17s 69us/step - loss: 0.0489 - acc: 0.9821 - val_loss: 0.0405 - val_acc: 0.9844\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 17s 69us/step - loss: 0.0472 - acc: 0.9826 - val_loss: 0.0415 - val_acc: 0.9844\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 17s 67us/step - loss: 0.0465 - acc: 0.9827 - val_loss: 0.0419 - val_acc: 0.9838\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 18s 71us/step - loss: 0.0456 - acc: 0.9831 - val_loss: 0.0405 - val_acc: 0.9851\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 17s 69us/step - loss: 0.0444 - acc: 0.9837 - val_loss: 0.0371 - val_acc: 0.9855\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 19s 74us/step - loss: 0.0445 - acc: 0.9835 - val_loss: 0.0382 - val_acc: 0.9853\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 17s 66us/step - loss: 0.0438 - acc: 0.9841 - val_loss: 0.0376 - val_acc: 0.9858\n",
      "LEP0.5-dr-128-l-1-de-1537064005\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 18s 71us/step - loss: 0.1313 - acc: 0.9540 - val_loss: 0.0588 - val_acc: 0.9771\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 17s 68us/step - loss: 0.0684 - acc: 0.9758 - val_loss: 0.0513 - val_acc: 0.9808\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 17s 68us/step - loss: 0.0609 - acc: 0.9782 - val_loss: 0.0473 - val_acc: 0.9829\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 18s 71us/step - loss: 0.0584 - acc: 0.9794 - val_loss: 0.0448 - val_acc: 0.9833\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 19s 74us/step - loss: 0.0564 - acc: 0.9795 - val_loss: 0.0436 - val_acc: 0.9833\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 17s 67us/step - loss: 0.0557 - acc: 0.9800 - val_loss: 0.0471 - val_acc: 0.9823\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 21s 83us/step - loss: 0.0544 - acc: 0.9804 - val_loss: 0.0424 - val_acc: 0.9845\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 21s 83us/step - loss: 0.0540 - acc: 0.9806 - val_loss: 0.0419 - val_acc: 0.9842\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 18s 73us/step - loss: 0.0533 - acc: 0.9810 - val_loss: 0.0418 - val_acc: 0.9838\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 19s 74us/step - loss: 0.0528 - acc: 0.9812 - val_loss: 0.0437 - val_acc: 0.9828\n",
      "LEP0.1-dr-32-l-9-de-1537064191\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 32s 127us/step - loss: 0.1822 - acc: 0.9365 - val_loss: 0.0939 - val_acc: 0.9663\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 27s 105us/step - loss: 0.0851 - acc: 0.9732 - val_loss: 0.0713 - val_acc: 0.9762\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 25s 98us/step - loss: 0.0697 - acc: 0.9768 - val_loss: 0.0525 - val_acc: 0.9813\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 25s 98us/step - loss: 0.0632 - acc: 0.9785 - val_loss: 0.0502 - val_acc: 0.9816\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 26s 103us/step - loss: 0.0615 - acc: 0.9791 - val_loss: 0.0488 - val_acc: 0.9820\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 27s 108us/step - loss: 0.0585 - acc: 0.9797 - val_loss: 0.0479 - val_acc: 0.9826\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 25s 101us/step - loss: 0.0588 - acc: 0.9798 - val_loss: 0.0488 - val_acc: 0.9826\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 26s 105us/step - loss: 0.0567 - acc: 0.9806 - val_loss: 0.0492 - val_acc: 0.9835\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 26s 103us/step - loss: 0.0554 - acc: 0.9810 - val_loss: 0.0483 - val_acc: 0.9827\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 27s 109us/step - loss: 0.0546 - acc: 0.9810 - val_loss: 0.0451 - val_acc: 0.9835\n",
      "LEP0.3-dr-32-l-9-de-1537064462\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 30s 121us/step - loss: 0.3119 - acc: 0.8810 - val_loss: 0.1287 - val_acc: 0.9654\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 27s 108us/step - loss: 0.1525 - acc: 0.9584 - val_loss: 0.1189 - val_acc: 0.9681\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 26s 104us/step - loss: 0.1286 - acc: 0.9640 - val_loss: 0.0953 - val_acc: 0.9736\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 26s 102us/step - loss: 0.1145 - acc: 0.9665 - val_loss: 0.0839 - val_acc: 0.9732\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 26s 103us/step - loss: 0.1084 - acc: 0.9676 - val_loss: 0.0894 - val_acc: 0.9730\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 27s 109us/step - loss: 0.1060 - acc: 0.9690 - val_loss: 0.0879 - val_acc: 0.9692\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 26s 102us/step - loss: 0.1059 - acc: 0.9692 - val_loss: 0.0919 - val_acc: 0.9693\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 27s 107us/step - loss: 0.1025 - acc: 0.9692 - val_loss: 0.0839 - val_acc: 0.9723\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 29s 114us/step - loss: 0.0998 - acc: 0.9701 - val_loss: 0.0783 - val_acc: 0.9744\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 26s 103us/step - loss: 0.1023 - acc: 0.9693 - val_loss: 0.1021 - val_acc: 0.9699\n",
      "LEP0.5-dr-32-l-9-de-1537064737\n",
      "Train on 251999 samples, validate on 108001 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 28s 111us/step - loss: 0.6712 - acc: 0.6955 - val_loss: 0.7809 - val_acc: 0.5075\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 26s 105us/step - loss: 0.3957 - acc: 0.8532 - val_loss: 0.9741 - val_acc: 0.5132\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 27s 108us/step - loss: 0.3573 - acc: 0.8798 - val_loss: 0.9445 - val_acc: 0.5070\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 26s 105us/step - loss: 0.3392 - acc: 0.8923 - val_loss: 1.2218 - val_acc: 0.5027\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 26s 105us/step - loss: 0.3212 - acc: 0.9038 - val_loss: 0.9491 - val_acc: 0.5063\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 27s 108us/step - loss: 0.3043 - acc: 0.9142 - val_loss: 1.2765 - val_acc: 0.4604\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 27s 108us/step - loss: 0.2953 - acc: 0.9209 - val_loss: 1.4881 - val_acc: 0.4935\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 28s 113us/step - loss: 0.2924 - acc: 0.9254 - val_loss: 1.3750 - val_acc: 0.5024\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 30s 120us/step - loss: 0.2733 - acc: 0.9303 - val_loss: 1.3112 - val_acc: 0.4991\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 31s 122us/step - loss: 0.2740 - acc: 0.9309 - val_loss: 1.4698 - val_acc: 0.4979\n",
      "LEP0.1-dr-64-l-9-de-1537065020\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 40s 158us/step - loss: 0.1184 - acc: 0.9600 - val_loss: 0.0563 - val_acc: 0.9784\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 39s 157us/step - loss: 0.0627 - acc: 0.9787 - val_loss: 0.0498 - val_acc: 0.9821\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 38s 151us/step - loss: 0.0568 - acc: 0.9809 - val_loss: 0.0488 - val_acc: 0.9824\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 39s 154us/step - loss: 0.0544 - acc: 0.9815 - val_loss: 0.0474 - val_acc: 0.9822\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 39s 154us/step - loss: 0.0512 - acc: 0.9824 - val_loss: 0.0441 - val_acc: 0.9846\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 38s 152us/step - loss: 0.0508 - acc: 0.9824 - val_loss: 0.0430 - val_acc: 0.9838\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 39s 155us/step - loss: 0.0493 - acc: 0.9829 - val_loss: 0.0422 - val_acc: 0.9847\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 40s 157us/step - loss: 0.0509 - acc: 0.9830 - val_loss: 0.0418 - val_acc: 0.9852\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 39s 156us/step - loss: 0.0517 - acc: 0.9832 - val_loss: 0.0469 - val_acc: 0.9822\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 39s 153us/step - loss: 0.0487 - acc: 0.9835 - val_loss: 0.0525 - val_acc: 0.9810\n",
      "LEP0.3-dr-64-l-9-de-1537065418\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 39s 156us/step - loss: 0.2687 - acc: 0.9028 - val_loss: 0.1309 - val_acc: 0.9580\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 39s 154us/step - loss: 0.1220 - acc: 0.9644 - val_loss: 0.0658 - val_acc: 0.9776\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 38s 151us/step - loss: 0.0993 - acc: 0.9701 - val_loss: 0.0625 - val_acc: 0.9793\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 39s 156us/step - loss: 0.0916 - acc: 0.9715 - val_loss: 0.0668 - val_acc: 0.9763\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 38s 151us/step - loss: 0.0895 - acc: 0.9718 - val_loss: 0.0618 - val_acc: 0.9743\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 38s 152us/step - loss: 0.0884 - acc: 0.9726 - val_loss: 0.0600 - val_acc: 0.9776\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 39s 154us/step - loss: 0.0868 - acc: 0.9732 - val_loss: 0.0586 - val_acc: 0.9764\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 38s 152us/step - loss: 0.0875 - acc: 0.9732 - val_loss: 0.0606 - val_acc: 0.9742\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 39s 155us/step - loss: 0.0846 - acc: 0.9741 - val_loss: 0.0618 - val_acc: 0.9754\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 38s 151us/step - loss: 0.0867 - acc: 0.9737 - val_loss: 0.0640 - val_acc: 0.9772\n",
      "LEP0.5-dr-64-l-9-de-1537065814\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 42s 166us/step - loss: 0.5429 - acc: 0.7855 - val_loss: 1.5621 - val_acc: 0.2920\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 41s 162us/step - loss: 0.3432 - acc: 0.8913 - val_loss: 1.7173 - val_acc: 0.2776\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 41s 162us/step - loss: 0.3000 - acc: 0.9150 - val_loss: 2.4757 - val_acc: 0.3776\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 41s 163us/step - loss: 0.2599 - acc: 0.9304 - val_loss: 0.9662 - val_acc: 0.6775\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 40s 159us/step - loss: 0.2376 - acc: 0.9407 - val_loss: 0.7553 - val_acc: 0.6907\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 41s 162us/step - loss: 0.2331 - acc: 0.9449 - val_loss: 0.6056 - val_acc: 0.7256\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 40s 159us/step - loss: 0.2336 - acc: 0.9445 - val_loss: 0.8938 - val_acc: 0.7251\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 40s 158us/step - loss: 0.2286 - acc: 0.9467 - val_loss: 0.8241 - val_acc: 0.7133\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 41s 162us/step - loss: 0.2220 - acc: 0.9481 - val_loss: 0.7027 - val_acc: 0.7288\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 40s 158us/step - loss: 0.2237 - acc: 0.9483 - val_loss: 0.5595 - val_acc: 0.7364\n",
      "LEP0.1-dr-128-l-9-de-1537066230\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 75s 297us/step - loss: 0.1085 - acc: 0.9637 - val_loss: 0.0581 - val_acc: 0.9777\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 76s 300us/step - loss: 0.0632 - acc: 0.9781 - val_loss: 0.0522 - val_acc: 0.9809\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 65s 260us/step - loss: 0.0667 - acc: 0.9800 - val_loss: 0.0469 - val_acc: 0.9833\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 75s 299us/step - loss: 0.0568 - acc: 0.9810 - val_loss: 0.0479 - val_acc: 0.9831\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 68s 271us/step - loss: 0.0606 - acc: 0.9810 - val_loss: 0.0452 - val_acc: 0.9839\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 68s 269us/step - loss: 0.0593 - acc: 0.9815 - val_loss: 0.0472 - val_acc: 0.9835\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 64s 254us/step - loss: 0.0591 - acc: 0.9818 - val_loss: 0.0428 - val_acc: 0.9847\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 63s 251us/step - loss: 0.0725 - acc: 0.9814 - val_loss: 0.1568 - val_acc: 0.9779\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 64s 252us/step - loss: 0.1259 - acc: 0.9784 - val_loss: 0.0504 - val_acc: 0.9776\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 64s 255us/step - loss: 0.1356 - acc: 0.9797 - val_loss: 0.1955 - val_acc: 0.9785\n",
      "LEP0.3-dr-128-l-9-de-1537066924\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 70s 277us/step - loss: 0.1582 - acc: 0.9471 - val_loss: 0.0669 - val_acc: 0.9766\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 68s 269us/step - loss: 0.0843 - acc: 0.9733 - val_loss: 0.0643 - val_acc: 0.9783\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 67s 265us/step - loss: 0.0803 - acc: 0.9743 - val_loss: 0.0558 - val_acc: 0.9796\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 69s 274us/step - loss: 0.0795 - acc: 0.9747 - val_loss: 0.0729 - val_acc: 0.9748\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 71s 283us/step - loss: 0.0779 - acc: 0.9756 - val_loss: 0.0685 - val_acc: 0.9739\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 63s 251us/step - loss: 0.0772 - acc: 0.9761 - val_loss: 0.0570 - val_acc: 0.9809\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 64s 254us/step - loss: 0.0783 - acc: 0.9762 - val_loss: 0.0578 - val_acc: 0.9798\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 62s 247us/step - loss: 0.0908 - acc: 0.9754 - val_loss: 0.0594 - val_acc: 0.9803\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 63s 250us/step - loss: 0.0812 - acc: 0.9756 - val_loss: 0.0735 - val_acc: 0.9758\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 63s 250us/step - loss: 0.0804 - acc: 0.9762 - val_loss: 0.0643 - val_acc: 0.9790\n",
      "LEP0.5-dr-128-l-9-de-1537067594\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 67s 267us/step - loss: 0.2971 - acc: 0.8971 - val_loss: 0.1978 - val_acc: 0.9610\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 67s 265us/step - loss: 0.1564 - acc: 0.9613 - val_loss: 0.1554 - val_acc: 0.9659\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 66s 260us/step - loss: 0.1387 - acc: 0.9654 - val_loss: 0.2178 - val_acc: 0.9522\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 67s 264us/step - loss: 0.1347 - acc: 0.9668 - val_loss: 0.2238 - val_acc: 0.9619\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 65s 258us/step - loss: 0.1340 - acc: 0.9671 - val_loss: 0.1552 - val_acc: 0.9701\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 67s 265us/step - loss: 0.1332 - acc: 0.9674 - val_loss: 0.1702 - val_acc: 0.9670\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 66s 262us/step - loss: 0.1335 - acc: 0.9676 - val_loss: 0.2288 - val_acc: 0.9630\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 65s 259us/step - loss: 0.1384 - acc: 0.9673 - val_loss: 0.1346 - val_acc: 0.9695\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 67s 265us/step - loss: 0.1374 - acc: 0.9671 - val_loss: 0.1924 - val_acc: 0.9611\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 72s 286us/step - loss: 0.1382 - acc: 0.9666 - val_loss: 0.1970 - val_acc: 0.9589\n",
      "LEP0.1-dr-32-l-27-de-1537068271\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 72s 284us/step - loss: 0.3297 - acc: 0.8836 - val_loss: 0.6005 - val_acc: 0.8117\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 65s 256us/step - loss: 0.1947 - acc: 0.9458 - val_loss: 0.4765 - val_acc: 0.8541\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 73s 289us/step - loss: 0.1440 - acc: 0.9625 - val_loss: 0.2931 - val_acc: 0.9513\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 63s 252us/step - loss: 0.1451 - acc: 0.9605 - val_loss: 0.4559 - val_acc: 0.8556\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 64s 254us/step - loss: 0.1117 - acc: 0.9700 - val_loss: 0.3000 - val_acc: 0.9311\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 59s 234us/step - loss: 0.1535 - acc: 0.9569 - val_loss: 0.4093 - val_acc: 0.8887\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 62s 247us/step - loss: 0.1511 - acc: 0.9610 - val_loss: 1.1774 - val_acc: 0.4572\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 61s 243us/step - loss: 0.1183 - acc: 0.9705 - val_loss: 1.1396 - val_acc: 0.3310\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 76s 301us/step - loss: 0.1127 - acc: 0.9688 - val_loss: 0.2748 - val_acc: 0.9508\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 78s 311us/step - loss: 0.1508 - acc: 0.9627 - val_loss: 1.1525 - val_acc: 0.3280\n",
      "LEP0.3-dr-32-l-27-de-1537068958\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 75s 296us/step - loss: 0.9183 - acc: 0.5075 - val_loss: 1.2058 - val_acc: 0.2817\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 69s 272us/step - loss: 0.3349 - acc: 0.8882 - val_loss: 2.7562 - val_acc: 0.2656\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 66s 263us/step - loss: 0.2493 - acc: 0.9382 - val_loss: 3.0118 - val_acc: 0.2599\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 68s 271us/step - loss: 0.3257 - acc: 0.9003 - val_loss: 2.6037 - val_acc: 0.2857\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 69s 275us/step - loss: 0.2984 - acc: 0.9372 - val_loss: 2.5648 - val_acc: 0.4945\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 67s 267us/step - loss: 0.2717 - acc: 0.9426 - val_loss: 2.5508 - val_acc: 0.4924\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 70s 276us/step - loss: 0.2650 - acc: 0.9435 - val_loss: 2.6426 - val_acc: 0.4938\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 70s 277us/step - loss: 0.5353 - acc: 0.9199 - val_loss: 2.6057 - val_acc: 0.4971\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 70s 276us/step - loss: 0.3711 - acc: 0.9292 - val_loss: 2.5943 - val_acc: 0.4899\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 70s 280us/step - loss: 0.9161 - acc: 0.8966 - val_loss: 6.2307 - val_acc: 0.2500\n",
      "LEP0.5-dr-32-l-27-de-1537069672\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 77s 304us/step - loss: 0.9661 - acc: 0.4936 - val_loss: 2.6977 - val_acc: 0.2498\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 67s 267us/step - loss: 0.8421 - acc: 0.5448 - val_loss: 3.0471 - val_acc: 0.2508\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 59s 233us/step - loss: 0.8014 - acc: 0.5862 - val_loss: 2.0479 - val_acc: 0.2539\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 62s 246us/step - loss: 0.7754 - acc: 0.6247 - val_loss: 2.0352 - val_acc: 0.2542\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 58s 229us/step - loss: 0.7729 - acc: 0.6319 - val_loss: 2.0412 - val_acc: 0.2542\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 59s 233us/step - loss: 0.7653 - acc: 0.6445 - val_loss: 1.8998 - val_acc: 0.2545\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 59s 233us/step - loss: 0.7625 - acc: 0.6546 - val_loss: 1.8283 - val_acc: 0.2548\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 59s 233us/step - loss: 0.7617 - acc: 0.6602 - val_loss: 1.8881 - val_acc: 0.2544\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 59s 233us/step - loss: 0.7583 - acc: 0.6611 - val_loss: 1.7905 - val_acc: 0.2550\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 58s 229us/step - loss: 0.7611 - acc: 0.6663 - val_loss: 1.8171 - val_acc: 0.2550\n",
      "LEP0.1-dr-64-l-27-de-1537070310\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 86s 340us/step - loss: 0.2679 - acc: 0.9098 - val_loss: 0.2435 - val_acc: 0.8834\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 82s 325us/step - loss: 0.1974 - acc: 0.9413 - val_loss: 0.2266 - val_acc: 0.9402\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 80s 318us/step - loss: 0.1615 - acc: 0.9557 - val_loss: 0.2103 - val_acc: 0.9510\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 89s 352us/step - loss: 0.1396 - acc: 0.9624 - val_loss: 0.6361 - val_acc: 0.7415\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 86s 342us/step - loss: 0.1235 - acc: 0.9682 - val_loss: 0.3282 - val_acc: 0.9478\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 86s 340us/step - loss: 0.1206 - acc: 0.9687 - val_loss: 0.4919 - val_acc: 0.8810\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 88s 350us/step - loss: 0.1603 - acc: 0.9630 - val_loss: 0.3095 - val_acc: 0.9473\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 90s 356us/step - loss: 0.1300 - acc: 0.9681 - val_loss: 0.4892 - val_acc: 0.7898\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 85s 337us/step - loss: 0.1628 - acc: 0.9519 - val_loss: 1.2641 - val_acc: 0.3691\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 88s 349us/step - loss: 0.1351 - acc: 0.9673 - val_loss: 0.5891 - val_acc: 0.6654\n",
      "LEP0.3-dr-64-l-27-de-1537071185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 88s 349us/step - loss: 0.6464 - acc: 0.6563 - val_loss: 0.8611 - val_acc: 0.6196\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 95s 376us/step - loss: 0.5401 - acc: 0.7067 - val_loss: 0.6295 - val_acc: 0.6882\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 93s 370us/step - loss: 0.5058 - acc: 0.7156 - val_loss: 0.5999 - val_acc: 0.7042\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 88s 348us/step - loss: 0.5028 - acc: 0.7159 - val_loss: 0.6478 - val_acc: 0.7033\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 82s 325us/step - loss: 0.5109 - acc: 0.7123 - val_loss: 0.5480 - val_acc: 0.7128\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 90s 358us/step - loss: 0.5013 - acc: 0.7163 - val_loss: 1.2329 - val_acc: 0.5085\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 97s 384us/step - loss: 0.5131 - acc: 0.7106 - val_loss: 0.5872 - val_acc: 0.7043\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 92s 366us/step - loss: 0.5241 - acc: 0.7082 - val_loss: 0.5564 - val_acc: 0.7097\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 104s 414us/step - loss: 0.5303 - acc: 0.7069 - val_loss: 0.7046 - val_acc: 0.6965\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 110s 436us/step - loss: 0.5185 - acc: 0.7114 - val_loss: 0.6108 - val_acc: 0.6966\n",
      "LEP0.5-dr-64-l-27-de-1537072143\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 112s 443us/step - loss: 0.7860 - acc: 0.6667 - val_loss: 3.7461 - val_acc: 0.2508\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 117s 463us/step - loss: 0.5995 - acc: 0.7687 - val_loss: 3.6272 - val_acc: 0.2509\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 128s 510us/step - loss: 0.5953 - acc: 0.7925 - val_loss: 3.0907 - val_acc: 0.2508\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 149s 590us/step - loss: 0.6043 - acc: 0.7986 - val_loss: 2.4962 - val_acc: 0.2508\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 141s 558us/step - loss: 0.6465 - acc: 0.7844 - val_loss: 2.2040 - val_acc: 0.2548\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 107s 426us/step - loss: 0.7062 - acc: 0.7614 - val_loss: 2.1536 - val_acc: 0.2549\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 93s 368us/step - loss: 0.7544 - acc: 0.7284 - val_loss: 2.2650 - val_acc: 0.2558\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 95s 377us/step - loss: 0.8113 - acc: 0.6758 - val_loss: 2.2615 - val_acc: 0.2519\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 102s 404us/step - loss: 0.8409 - acc: 0.6562 - val_loss: 2.1563 - val_acc: 0.2547\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 99s 394us/step - loss: 0.8998 - acc: 0.6316 - val_loss: 1.8192 - val_acc: 0.2541\n",
      "LEP0.1-dr-128-l-27-de-1537073319\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 271s 1ms/step - loss: 0.2877 - acc: 0.8981 - val_loss: 0.1547 - val_acc: 0.9685\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 261s 1ms/step - loss: 0.2520 - acc: 0.9431 - val_loss: 0.3997 - val_acc: 0.8745\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 261s 1ms/step - loss: 0.1777 - acc: 0.9535 - val_loss: 0.5235 - val_acc: 0.7556\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 261s 1ms/step - loss: 0.3044 - acc: 0.9615 - val_loss: 0.3966 - val_acc: 0.8558\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 270s 1ms/step - loss: 0.2285 - acc: 0.9653 - val_loss: 0.2974 - val_acc: 0.9508\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 273s 1ms/step - loss: 0.3009 - acc: 0.9558 - val_loss: 0.9301 - val_acc: 0.5353\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 285s 1ms/step - loss: 0.1622 - acc: 0.9644 - val_loss: 1.0504 - val_acc: 0.4874\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 290s 1ms/step - loss: 0.1384 - acc: 0.9594 - val_loss: 0.7155 - val_acc: 0.6506\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 291s 1ms/step - loss: 0.1526 - acc: 0.9553 - val_loss: 0.7480 - val_acc: 0.6034\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 281s 1ms/step - loss: 0.1782 - acc: 0.9546 - val_loss: 0.9072 - val_acc: 0.6391\n",
      "LEP0.3-dr-128-l-27-de-1537076088\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 296s 1ms/step - loss: 0.6376 - acc: 0.6671 - val_loss: 0.6604 - val_acc: 0.6603\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 289s 1ms/step - loss: 0.3269 - acc: 0.8721 - val_loss: 0.3622 - val_acc: 0.9189\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 309s 1ms/step - loss: 0.2665 - acc: 0.9296 - val_loss: 0.2691 - val_acc: 0.9311\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 319s 1ms/step - loss: 0.2717 - acc: 0.9310 - val_loss: 0.7393 - val_acc: 0.6521\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 319s 1ms/step - loss: 0.5017 - acc: 0.8878 - val_loss: 0.8007 - val_acc: 0.6715\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 319s 1ms/step - loss: 0.3713 - acc: 0.9046 - val_loss: 1.0073 - val_acc: 0.7192\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 324s 1ms/step - loss: 0.7848 - acc: 0.8682 - val_loss: 0.6052 - val_acc: 0.7031\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 290s 1ms/step - loss: 1.8590 - acc: 0.6566 - val_loss: 1.1165 - val_acc: 0.4742\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 269s 1ms/step - loss: 0.7885 - acc: 0.6980 - val_loss: 1.5279 - val_acc: 0.3904\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 267s 1ms/step - loss: 0.6474 - acc: 0.7672 - val_loss: 1.1201 - val_acc: 0.5067\n",
      "LEP0.5-dr-128-l-27-de-1537079126\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 305s 1ms/step - loss: 0.8771 - acc: 0.4922 - val_loss: 1.0866 - val_acc: 0.3912\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 315s 1ms/step - loss: 0.8629 - acc: 0.4955 - val_loss: 0.9257 - val_acc: 0.4770\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 313s 1ms/step - loss: 0.8716 - acc: 0.4965 - val_loss: 0.8554 - val_acc: 0.4928\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 313s 1ms/step - loss: 0.8855 - acc: 0.4959 - val_loss: 0.8631 - val_acc: 0.4904\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 312s 1ms/step - loss: 0.8876 - acc: 0.4931 - val_loss: 0.8894 - val_acc: 0.4859\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 315s 1ms/step - loss: 0.8747 - acc: 0.4947 - val_loss: 0.8568 - val_acc: 0.4963\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 319s 1ms/step - loss: 0.8991 - acc: 0.4942 - val_loss: 0.8622 - val_acc: 0.4913\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 313s 1ms/step - loss: 0.8832 - acc: 0.4947 - val_loss: 0.8909 - val_acc: 0.4814\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 312s 1ms/step - loss: 0.9003 - acc: 0.4927 - val_loss: 0.9337 - val_acc: 0.4691\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 313s 1ms/step - loss: 0.8900 - acc: 0.4922 - val_loss: 0.8524 - val_acc: 0.4937\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [1,9,27]\n",
    "layer_sizes = [32,64,128]\n",
    "conv_layers = [0.1,0.3,0.5]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"LEP{}-dr-{}-l-{}-de-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Dense(layer_size,input_shape = (14,)))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Dropout(conv_layer))                                       # dropout 10% of the neurons\n",
    "#           model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#           model.add(Flatten())\n",
    "            for l in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(Dropout(conv_layer))# dropout 10% of the neurons\n",
    "            \n",
    "            model.add(Dense(4))\n",
    "            model.add(Activation('softmax'))\n",
    "            \n",
    "            tensorboard = TensorBoard(log_dir = 'log/{}'.format(NAME))\n",
    "\n",
    "##qual otimizador?\n",
    "            model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "            model.fit(X_train, y_train,\n",
    "                      batch_size=32,\n",
    "                      epochs=10,\n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[tensorboard])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De acordo com os graficos de val_loss, val_acc, escolhemos o seguintes modelos:\n",
    "## LEP0.1-dr-128-l-1-de-1537063651\n",
    "## LEP0.1-dr-64-l-1-de-1537063247\n",
    "## LEP0.3-dr-128-l-1-de-1537063827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst-LEP-0.1-dr-128-l-1-de\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/30\n",
      "251999/251999 [==============================] - 76s 303us/step - loss: 0.0868 - acc: 0.9695 - val_loss: 0.0520 - val_acc: 0.9808\n",
      "Epoch 2/30\n",
      "251999/251999 [==============================] - 68s 269us/step - loss: 0.0515 - acc: 0.9807 - val_loss: 0.0468 - val_acc: 0.9826\n",
      "Epoch 3/30\n",
      "251999/251999 [==============================] - 63s 248us/step - loss: 0.0453 - acc: 0.9829 - val_loss: 0.0415 - val_acc: 0.9844\n",
      "Epoch 4/30\n",
      "251999/251999 [==============================] - 72s 284us/step - loss: 0.0429 - acc: 0.9839 - val_loss: 0.0393 - val_acc: 0.9857\n",
      "Epoch 5/30\n",
      "251999/251999 [==============================] - 60s 236us/step - loss: 0.0413 - acc: 0.9843 - val_loss: 0.0397 - val_acc: 0.9848\n",
      "Epoch 6/30\n",
      "251999/251999 [==============================] - 63s 250us/step - loss: 0.0400 - acc: 0.9850 - val_loss: 0.0389 - val_acc: 0.9846\n",
      "Epoch 7/30\n",
      "251999/251999 [==============================] - 60s 237us/step - loss: 0.0394 - acc: 0.9851 - val_loss: 0.0378 - val_acc: 0.9859\n",
      "Epoch 8/30\n",
      "251999/251999 [==============================] - 57s 227us/step - loss: 0.0387 - acc: 0.9854 - val_loss: 0.0362 - val_acc: 0.9859\n",
      "Epoch 9/30\n",
      "251999/251999 [==============================] - 64s 252us/step - loss: 0.0388 - acc: 0.9855 - val_loss: 0.0367 - val_acc: 0.9861\n",
      "Epoch 10/30\n",
      "251999/251999 [==============================] - 64s 254us/step - loss: 0.0377 - acc: 0.9858 - val_loss: 0.0365 - val_acc: 0.9860\n",
      "Epoch 11/30\n",
      "251999/251999 [==============================] - 60s 238us/step - loss: 0.0374 - acc: 0.9858 - val_loss: 0.0368 - val_acc: 0.9865\n",
      "Epoch 12/30\n",
      "251999/251999 [==============================] - 63s 249us/step - loss: 0.0371 - acc: 0.9859 - val_loss: 0.0382 - val_acc: 0.9850\n",
      "Epoch 13/30\n",
      "251999/251999 [==============================] - 60s 238us/step - loss: 0.0364 - acc: 0.9863 - val_loss: 0.0361 - val_acc: 0.9863\n",
      "Epoch 14/30\n",
      "251999/251999 [==============================] - 61s 241us/step - loss: 0.0370 - acc: 0.9862 - val_loss: 0.0382 - val_acc: 0.9855\n",
      "Epoch 15/30\n",
      "251999/251999 [==============================] - 64s 252us/step - loss: 0.0363 - acc: 0.9864 - val_loss: 0.0349 - val_acc: 0.9867\n",
      "Epoch 16/30\n",
      "251999/251999 [==============================] - 60s 240us/step - loss: 0.0356 - acc: 0.9865 - val_loss: 0.0346 - val_acc: 0.9866\n",
      "Epoch 17/30\n",
      "251999/251999 [==============================] - 60s 238us/step - loss: 0.0361 - acc: 0.9865 - val_loss: 0.0347 - val_acc: 0.9864\n",
      "Epoch 18/30\n",
      "251999/251999 [==============================] - 64s 255us/step - loss: 0.0359 - acc: 0.9865 - val_loss: 0.0345 - val_acc: 0.9866\n",
      "Epoch 19/30\n",
      "251999/251999 [==============================] - 63s 250us/step - loss: 0.0356 - acc: 0.9866 - val_loss: 0.0350 - val_acc: 0.9865\n",
      "Epoch 20/30\n",
      "251999/251999 [==============================] - 62s 245us/step - loss: 0.0355 - acc: 0.9867 - val_loss: 0.0343 - val_acc: 0.9870\n",
      "Epoch 21/30\n",
      "251999/251999 [==============================] - 61s 240us/step - loss: 0.0355 - acc: 0.9866 - val_loss: 0.0341 - val_acc: 0.9867\n",
      "Epoch 22/30\n",
      "251999/251999 [==============================] - 63s 249us/step - loss: 0.0348 - acc: 0.9867 - val_loss: 0.0345 - val_acc: 0.9870\n",
      "Epoch 23/30\n",
      "251999/251999 [==============================] - 63s 249us/step - loss: 0.0349 - acc: 0.9867 - val_loss: 0.0332 - val_acc: 0.9870\n",
      "Epoch 24/30\n",
      "251999/251999 [==============================] - 61s 241us/step - loss: 0.0348 - acc: 0.9868 - val_loss: 0.0353 - val_acc: 0.9866\n",
      "Epoch 25/30\n",
      "251999/251999 [==============================] - 62s 245us/step - loss: 0.0346 - acc: 0.9870 - val_loss: 0.0352 - val_acc: 0.9870\n",
      "Epoch 26/30\n",
      "251999/251999 [==============================] - 60s 238us/step - loss: 0.0344 - acc: 0.9869 - val_loss: 0.0348 - val_acc: 0.9862\n",
      "Epoch 27/30\n",
      "251999/251999 [==============================] - 60s 236us/step - loss: 0.0348 - acc: 0.9869 - val_loss: 0.0331 - val_acc: 0.9876\n",
      "Epoch 28/30\n",
      "251999/251999 [==============================] - 66s 263us/step - loss: 0.0339 - acc: 0.9872 - val_loss: 0.0348 - val_acc: 0.9867\n",
      "Epoch 29/30\n",
      "251999/251999 [==============================] - 61s 242us/step - loss: 0.0342 - acc: 0.9870 - val_loss: 0.0360 - val_acc: 0.9870\n",
      "Epoch 30/30\n",
      "251999/251999 [==============================] - 62s 245us/step - loss: 0.0338 - acc: 0.9873 - val_loss: 0.0337 - val_acc: 0.9866\n"
     ]
    }
   ],
   "source": [
    "dense_layer = 1\n",
    "layer_size = 128\n",
    "drop_layer = 0.1\n",
    "\n",
    "\n",
    "NAME1 = \"fst-LEP-{}-dr-{}-l-{}-de\".format(drop_layer, layer_size, dense_layer)\n",
    "print(NAME1)\n",
    "tensorboard = TensorBoard(log_dir = 'log/{}'.format(NAME1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(layer_size,input_shape = (int(14),)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(drop_layer))                                       # dropout 10% of the neurons\n",
    "#           model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#           model.add(Flatten())\n",
    "for l in range(dense_layer):\n",
    "    model.add(Dense(layer_size))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop_layer))# dropout 10% of the neurons\n",
    "\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir = 'log/{}'.format(NAME_final))\n",
    "\n",
    "#qual otimizador?\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "LEP_model = model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=30,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 5s 131us/step\n",
      "Loss: 0.03463609146872768\n",
      "Accuracy: 0.9864\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test,y_test)\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fst-LEP-0.1-dr-128-l-1-de.model'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomemodelo = str(NAME1)+'.model'\n",
    "nomemodelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fst-LEP-0.1-dr-128-l-1-de.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEP_new_model = tf.keras.models.load_model('fst-LEP-0.1-dr-128-l-1-de.model') #loading the model\n",
    "\n",
    "predictions = LEP_new_model.predict([X_test]) #testing the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "itt= random.randint(0,len(y_test))\n",
    "itt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: Muon\n"
     ]
    }
   ],
   "source": [
    "print('Event:', CATEGORIES[y_test[itt]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Muon\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Prediction:',CATEGORIES[np.argmax((predictions[itt]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff for Electron :0.9957639939485627\n",
      "Eff for Muon :0.9901970591177354\n",
      "Eff for Tau :0.9637068708362335\n",
      "Eff for Quark :0.9961120526368259\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'y_test': y_test})\n",
    "#df['y_pred'] = np.array(predictions)\n",
    "\n",
    "#creating a array with the predictions\n",
    "y_pred = [np.argmax(predictions[i]) for i in range(len(y_test))]\n",
    "y_prob = [predictions[i][np.argmax(predictions[i])] for i in range(len(y_test))]\n",
    "\n",
    "\n",
    "pd_y_test = pd.DataFrame({'y_test':y_test})\n",
    "pd_y_prob = pd.DataFrame({'y_prob':y_prob})\n",
    "pd_y_pred = pd.DataFrame({'y_pred':y_pred})\n",
    "\n",
    "df_final = pd.concat([pd_y_pred, pd_y_test,pd_y_prob], axis=1)\n",
    "\n",
    "#df_final.head(10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#evaluating the completeness\n",
    "#df_finalsh = df_final.sample(frac=1).reset_index(drop=True)  #shuffle the rows only\n",
    "\n",
    "df_final_0 = df_final[df_final['y_test'] == 0]\n",
    "df_final_1 = df_final[df_final['y_test'] == 1]\n",
    "df_final_2 = df_final[df_final['y_test'] == 2]\n",
    "df_final_3 = df_final[df_final['y_test'] == 3]\n",
    "\n",
    "n0 = df_final_0[df_final_0.y_pred == 0]    # correct predictions for class-0 / threshold=0\n",
    "n1 = df_final_1[df_final_1.y_pred == 1]    # correct predictions for class-1 / threshold=0\n",
    "n2 = df_final_2[df_final_2.y_pred == 2]    # correct predictions for class-2 / threshold=0\n",
    "n3 = df_final_3[df_final_3.y_pred == 3]\n",
    "\n",
    "\n",
    "n_0 = n0.count()                           #number of correct predictions for class-0 / threshold=0\n",
    "n_1 = n1.count()                           #number of correct predictions for class-1 / threshold=0\n",
    "n_2 = n2.count()\n",
    "n_3 = n3.count()\n",
    "#number of correct predictions for class-0 / threshold=0\n",
    "\n",
    "count_0 = df_final_0['y_test'].count()     #get the lenght of the sample of class-0,1 and 2\n",
    "count_1 = df_final_1['y_test'].count()\n",
    "count_2 = df_final_2['y_test'].count()\n",
    "count_3 = df_final_3['y_test'].count()\n",
    "\n",
    "print(\"Eff for {} :{}\" .format(CATEGORIES[0], n_0[0]/count_0))\n",
    "print(\"Eff for {} :{}\" .format(CATEGORIES[1], n_1[0]/count_1))\n",
    "print(\"Eff for {} :{}\" .format(CATEGORIES[2], n_2[0]/count_2))\n",
    "print(\"Eff for {} :{}\" .format(CATEGORIES[3], n_3[0]/count_3))\n",
    "#print(\"Efficiency_picnic:\",n_1[0]/count_1)\n",
    "#print(\"Efficiency_meeting:\",n_2[0]/count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Electron :9915\n",
      "Number of Muon :9997\n",
      "Number of Tau :10057\n",
      "Number of Quark :10031\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of {} :{}\" .format(CATEGORIES[0], count_0))\n",
    "print(\"Number of {} :{}\" .format(CATEGORIES[1], count_1))\n",
    "print(\"Number of {} :{}\" .format(CATEGORIES[2], count_2))\n",
    "print(\"Number of {} :{}\" .format(CATEGORIES[3], count_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
