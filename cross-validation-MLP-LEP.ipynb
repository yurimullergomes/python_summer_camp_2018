{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "CATEGORIES = ['Electron','Muon','Tau','Quark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"XLEP.pickle\",\"rb\")\n",
    "X0 = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"yLEP.pickle\",\"rb\")\n",
    "z0 = pickle.load(pickle_in)\n",
    "\n",
    "X = np.array(X0)\n",
    "\n",
    "y = np.array(z0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_data</th>\n",
       "      <th>X_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.1436767578125, 0.9298052191734314, 0.929267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[1.4359283447265625, 0.28194916248321533, 0.29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[2.599151611328125, 0.5472841858863831, 0.5643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.9006805419921875, -0.7204799652099609, -0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[1.2020416259765625, -0.5682700872421265, -0.5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_data                                             X_data\n",
       "0       0  [1.1436767578125, 0.9298052191734314, 0.929267...\n",
       "1       1  [1.4359283447265625, 0.28194916248321533, 0.29...\n",
       "2       3  [2.599151611328125, 0.5472841858863831, 0.5643...\n",
       "3       1  [0.9006805419921875, -0.7204799652099609, -0.7...\n",
       "4       2  [1.2020416259765625, -0.5682700872421265, -0.5..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_x = pd.DataFrame({'X_data': [X[i] for i in range(len(X))] })\n",
    "df_y = pd.DataFrame({'y_data': y })\n",
    "\n",
    "df_final = pd.concat([df_y,df_x], axis=1)\n",
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training K-times with shufled data and savind the predictions of each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 16s 64us/step - loss: 0.0878 - acc: 0.9687 - val_loss: 0.0515 - val_acc: 0.9809\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 16s 63us/step - loss: 0.0516 - acc: 0.9811 - val_loss: 0.0446 - val_acc: 0.9817\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 16s 63us/step - loss: 0.0455 - acc: 0.9829 - val_loss: 0.0435 - val_acc: 0.9841\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 16s 63us/step - loss: 0.0431 - acc: 0.9838 - val_loss: 0.0397 - val_acc: 0.9851\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 16s 63us/step - loss: 0.0416 - acc: 0.9844 - val_loss: 0.0410 - val_acc: 0.9837\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 16s 63us/step - loss: 0.0407 - acc: 0.9848 - val_loss: 0.0372 - val_acc: 0.9861\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 16s 62us/step - loss: 0.0398 - acc: 0.9852 - val_loss: 0.0358 - val_acc: 0.9865\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 16s 63us/step - loss: 0.0393 - acc: 0.9851 - val_loss: 0.0356 - val_acc: 0.9865\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 16s 62us/step - loss: 0.0386 - acc: 0.9854 - val_loss: 0.0360 - val_acc: 0.9863\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 16s 62us/step - loss: 0.0383 - acc: 0.9855 - val_loss: 0.0337 - val_acc: 0.9869\n",
      "40000/40000 [==============================] - 1s 17us/step\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 17s 67us/step - loss: 0.0921 - acc: 0.9677 - val_loss: 0.0527 - val_acc: 0.9800\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0521 - acc: 0.9806 - val_loss: 0.0451 - val_acc: 0.9834\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 16s 64us/step - loss: 0.0459 - acc: 0.9829 - val_loss: 0.0407 - val_acc: 0.9854\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 17s 69us/step - loss: 0.0434 - acc: 0.9835 - val_loss: 0.0453 - val_acc: 0.9831\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 17s 66us/step - loss: 0.0422 - acc: 0.9841 - val_loss: 0.0412 - val_acc: 0.9834\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 16s 64us/step - loss: 0.0412 - acc: 0.9844 - val_loss: 0.0368 - val_acc: 0.9867\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0405 - acc: 0.9848 - val_loss: 0.0385 - val_acc: 0.9862\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 16s 64us/step - loss: 0.0394 - acc: 0.9849 - val_loss: 0.0356 - val_acc: 0.9863\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 16s 64us/step - loss: 0.0388 - acc: 0.9852 - val_loss: 0.0349 - val_acc: 0.9872\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 16s 64us/step - loss: 0.0390 - acc: 0.9852 - val_loss: 0.0362 - val_acc: 0.9865\n",
      "40000/40000 [==============================] - 1s 16us/step\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 17s 67us/step - loss: 0.0848 - acc: 0.9697 - val_loss: 0.0535 - val_acc: 0.9808\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0507 - acc: 0.9811 - val_loss: 0.0439 - val_acc: 0.9836\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0446 - acc: 0.9833 - val_loss: 0.0439 - val_acc: 0.9834\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0427 - acc: 0.9838 - val_loss: 0.0394 - val_acc: 0.9858\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0413 - acc: 0.9844 - val_loss: 0.0380 - val_acc: 0.9853\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0404 - acc: 0.9846 - val_loss: 0.0376 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0395 - acc: 0.9850 - val_loss: 0.0361 - val_acc: 0.9868\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0392 - acc: 0.9852 - val_loss: 0.0359 - val_acc: 0.9866\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0385 - acc: 0.9854 - val_loss: 0.0339 - val_acc: 0.9875\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 16s 65us/step - loss: 0.0380 - acc: 0.9857 - val_loss: 0.0370 - val_acc: 0.9861\n",
      "40000/40000 [==============================] - 1s 18us/step\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 17s 68us/step - loss: 0.0879 - acc: 0.9688 - val_loss: 0.0548 - val_acc: 0.9790\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 17s 66us/step - loss: 0.0507 - acc: 0.9812 - val_loss: 0.0450 - val_acc: 0.9838\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 17s 66us/step - loss: 0.0448 - acc: 0.9830 - val_loss: 0.0430 - val_acc: 0.9834\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 17s 68us/step - loss: 0.0424 - acc: 0.9839 - val_loss: 0.0373 - val_acc: 0.9857\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 18s 70us/step - loss: 0.0412 - acc: 0.9844 - val_loss: 0.0378 - val_acc: 0.9861\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 17s 66us/step - loss: 0.0404 - acc: 0.9845 - val_loss: 0.0388 - val_acc: 0.9850\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 17s 66us/step - loss: 0.0395 - acc: 0.9852 - val_loss: 0.0363 - val_acc: 0.9863\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 17s 66us/step - loss: 0.0384 - acc: 0.9853 - val_loss: 0.0356 - val_acc: 0.9865\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 17s 66us/step - loss: 0.0379 - acc: 0.9856 - val_loss: 0.0351 - val_acc: 0.9869\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 17s 67us/step - loss: 0.0375 - acc: 0.9857 - val_loss: 0.0363 - val_acc: 0.9861\n",
      "40000/40000 [==============================] - 1s 19us/step\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 18s 71us/step - loss: 0.0902 - acc: 0.9683 - val_loss: 0.0549 - val_acc: 0.9802\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 18s 70us/step - loss: 0.0524 - acc: 0.9804 - val_loss: 0.0450 - val_acc: 0.9841\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 17s 69us/step - loss: 0.0460 - acc: 0.9828 - val_loss: 0.0441 - val_acc: 0.9822\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 17s 69us/step - loss: 0.0437 - acc: 0.9834 - val_loss: 0.0387 - val_acc: 0.9850\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 20s 77us/step - loss: 0.0417 - acc: 0.9842 - val_loss: 0.0396 - val_acc: 0.9848\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 22s 87us/step - loss: 0.0407 - acc: 0.9846 - val_loss: 0.0372 - val_acc: 0.9859\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0397 - acc: 0.9848 - val_loss: 0.0371 - val_acc: 0.9859\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0390 - acc: 0.9852 - val_loss: 0.0369 - val_acc: 0.9863\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 21s 85us/step - loss: 0.0390 - acc: 0.9854 - val_loss: 0.0371 - val_acc: 0.9866\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 22s 85us/step - loss: 0.0381 - acc: 0.9856 - val_loss: 0.0366 - val_acc: 0.9864\n",
      "40000/40000 [==============================] - 1s 26us/step\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 22s 87us/step - loss: 0.0879 - acc: 0.9685 - val_loss: 0.0509 - val_acc: 0.9814\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 22s 89us/step - loss: 0.0513 - acc: 0.9808 - val_loss: 0.0415 - val_acc: 0.9837\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0455 - acc: 0.9830 - val_loss: 0.0383 - val_acc: 0.9856\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 22s 89us/step - loss: 0.0429 - acc: 0.9840 - val_loss: 0.0416 - val_acc: 0.9841\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 23s 90us/step - loss: 0.0414 - acc: 0.9845 - val_loss: 0.0373 - val_acc: 0.9862\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0401 - acc: 0.9847 - val_loss: 0.0376 - val_acc: 0.9862\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 19s 77us/step - loss: 0.0393 - acc: 0.9854 - val_loss: 0.0450 - val_acc: 0.9827\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 19s 76us/step - loss: 0.0390 - acc: 0.9851 - val_loss: 0.0375 - val_acc: 0.9861\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 20s 80us/step - loss: 0.0386 - acc: 0.9856 - val_loss: 0.0350 - val_acc: 0.9868\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0380 - acc: 0.9856 - val_loss: 0.0353 - val_acc: 0.9866\n",
      "40000/40000 [==============================] - 1s 24us/step\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 20s 80us/step - loss: 0.0863 - acc: 0.9690 - val_loss: 0.0502 - val_acc: 0.9814\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 20s 79us/step - loss: 0.0501 - acc: 0.9812 - val_loss: 0.0426 - val_acc: 0.9839\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 20s 77us/step - loss: 0.0447 - acc: 0.9831 - val_loss: 0.0447 - val_acc: 0.9840\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 21s 82us/step - loss: 0.0429 - acc: 0.9839 - val_loss: 0.0411 - val_acc: 0.9845\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 23s 90us/step - loss: 0.0415 - acc: 0.9844 - val_loss: 0.0411 - val_acc: 0.9841\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 21s 81us/step - loss: 0.0401 - acc: 0.9851 - val_loss: 0.0351 - val_acc: 0.9863\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 24s 94us/step - loss: 0.0394 - acc: 0.9851 - val_loss: 0.0349 - val_acc: 0.9865\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 23s 90us/step - loss: 0.0388 - acc: 0.9853 - val_loss: 0.0361 - val_acc: 0.9861\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 21s 85us/step - loss: 0.0381 - acc: 0.9859 - val_loss: 0.0349 - val_acc: 0.9867\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 21s 85us/step - loss: 0.0380 - acc: 0.9858 - val_loss: 0.0337 - val_acc: 0.9868\n",
      "40000/40000 [==============================] - 1s 24us/step\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 23s 89us/step - loss: 0.0905 - acc: 0.9680 - val_loss: 0.0518 - val_acc: 0.9803\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 26s 102us/step - loss: 0.0513 - acc: 0.9810 - val_loss: 0.0419 - val_acc: 0.9838\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 24s 94us/step - loss: 0.0450 - acc: 0.9828 - val_loss: 0.0404 - val_acc: 0.9848\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 24s 96us/step - loss: 0.0427 - acc: 0.9841 - val_loss: 0.0403 - val_acc: 0.9849\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 21s 85us/step - loss: 0.0413 - acc: 0.9845 - val_loss: 0.0398 - val_acc: 0.9851\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 21s 84us/step - loss: 0.0400 - acc: 0.9849 - val_loss: 0.0383 - val_acc: 0.9851\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0396 - acc: 0.9850 - val_loss: 0.0445 - val_acc: 0.9826\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 23s 89us/step - loss: 0.0387 - acc: 0.9851 - val_loss: 0.0370 - val_acc: 0.9861\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 23s 93us/step - loss: 0.0381 - acc: 0.9855 - val_loss: 0.0381 - val_acc: 0.9856\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 24s 95us/step - loss: 0.0378 - acc: 0.9857 - val_loss: 0.0359 - val_acc: 0.9867\n",
      "40000/40000 [==============================] - 1s 36us/step\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 27s 108us/step - loss: 0.0896 - acc: 0.9680 - val_loss: 0.0485 - val_acc: 0.9820\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 23s 90us/step - loss: 0.0494 - acc: 0.9814 - val_loss: 0.0418 - val_acc: 0.9849\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 23s 91us/step - loss: 0.0446 - acc: 0.9833 - val_loss: 0.0387 - val_acc: 0.9854\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 22s 89us/step - loss: 0.0427 - acc: 0.9840 - val_loss: 0.0383 - val_acc: 0.9862\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 23s 92us/step - loss: 0.0410 - acc: 0.9845 - val_loss: 0.0413 - val_acc: 0.9841\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 22s 89us/step - loss: 0.0396 - acc: 0.9848 - val_loss: 0.0377 - val_acc: 0.9861\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 24s 94us/step - loss: 0.0389 - acc: 0.9850 - val_loss: 0.0373 - val_acc: 0.9863\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 22s 86us/step - loss: 0.0385 - acc: 0.9855 - val_loss: 0.0371 - val_acc: 0.9860\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 22s 88us/step - loss: 0.0380 - acc: 0.9856 - val_loss: 0.0364 - val_acc: 0.9862\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 24s 95us/step - loss: 0.0378 - acc: 0.9857 - val_loss: 0.0362 - val_acc: 0.9866\n",
      "40000/40000 [==============================] - 1s 29us/step\n",
      "Train on 251999 samples, validate on 108001 samples\n",
      "Epoch 1/10\n",
      "251999/251999 [==============================] - 23s 93us/step - loss: 0.0885 - acc: 0.9685 - val_loss: 0.0531 - val_acc: 0.9799\n",
      "Epoch 2/10\n",
      "251999/251999 [==============================] - 22s 89us/step - loss: 0.0515 - acc: 0.9804 - val_loss: 0.0463 - val_acc: 0.9833\n",
      "Epoch 3/10\n",
      "251999/251999 [==============================] - 22s 89us/step - loss: 0.0460 - acc: 0.9825 - val_loss: 0.0408 - val_acc: 0.9849\n",
      "Epoch 4/10\n",
      "251999/251999 [==============================] - 22s 87us/step - loss: 0.0433 - acc: 0.9835 - val_loss: 0.0404 - val_acc: 0.9854\n",
      "Epoch 5/10\n",
      "251999/251999 [==============================] - 22s 87us/step - loss: 0.0416 - acc: 0.9843 - val_loss: 0.0396 - val_acc: 0.9847\n",
      "Epoch 6/10\n",
      "251999/251999 [==============================] - 22s 88us/step - loss: 0.0407 - acc: 0.9847 - val_loss: 0.0374 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "251999/251999 [==============================] - 25s 98us/step - loss: 0.0396 - acc: 0.9849 - val_loss: 0.0369 - val_acc: 0.9858\n",
      "Epoch 8/10\n",
      "251999/251999 [==============================] - 21s 85us/step - loss: 0.0392 - acc: 0.9849 - val_loss: 0.0376 - val_acc: 0.9859\n",
      "Epoch 9/10\n",
      "251999/251999 [==============================] - 19s 75us/step - loss: 0.0390 - acc: 0.9853 - val_loss: 0.0372 - val_acc: 0.9853\n",
      "Epoch 10/10\n",
      "251999/251999 [==============================] - 18s 72us/step - loss: 0.0377 - acc: 0.9858 - val_loss: 0.0377 - val_acc: 0.9864\n",
      "40000/40000 [==============================] - 1s 22us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # for a sequential model \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "K=10\n",
    "dense_layer = 1\n",
    "layer_size = 128\n",
    "drop_layer = 0.1\n",
    "\n",
    "pred_cross = {}\n",
    "score = []\n",
    "#'fst-LEP-0.1-dr-128-l-1-de.model'\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for i in range(K):\n",
    "    df_final = df_final.sample(frac=1).reset_index(drop=True)  #shuffle the rows only\n",
    "    X_u = df_final['X_data']\n",
    "    y_u = df_final['y_data']\n",
    "    X_test = np.array([X_u[j] for j in range((len(X)//(K)))])\n",
    "    X_train = np.array([X_u[j] for j in range((len(X)//(K)),len(X))])\n",
    "\n",
    "    y_test = np.array([y_u[j] for j in range((len(y)//(K)))])\n",
    "    y_train = np.array([y_u[j] for j in range((len(y)//(K)),len(y))])\n",
    "    # Fit only to the training data\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "#    \n",
    "    NAMEk = \"{}-fold-{}-dr-{}-l-{}-de-{}\".format(K,drop_layer, layer_size, dense_layer, int(time.time()))\n",
    "    tensorboard = TensorBoard(log_dir = 'log/{}'.format(NAMEk))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer_size,input_shape = (int(14),)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop_layer))                                       # dropout 10% of the neurons\n",
    "    #           model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #           model.add(Flatten())\n",
    "    for l in range(dense_layer):\n",
    "        model.add(Dense(layer_size))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(drop_layer))# dropout 10% of the neurons\n",
    "\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #tensorboard = TensorBoard(log_dir = 'log/{}'.format(NAME_final))\n",
    "\n",
    "    #qual otimizador?\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "    LEP_model = model.fit(X_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              validation_split=0.3,\n",
    "              callbacks=[tensorboard])\n",
    "\n",
    "    pred_cross[str(i)+'pred'] = model.predict(X_test)\n",
    "    score.append(model.evaluate(X_test,y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc for K=10: 0.9866800000000001+-0.0007355100271240353\n"
     ]
    }
   ],
   "source": [
    "loss = [score[i][0] for i in range(K)]\n",
    "acc = [score[i][1] for i in range(K)]\n",
    "\n",
    "print('acc for K={}: {}+-{}'.format(K,np.average(acc),np.std(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 8.0746402e-18, 1.0000000e+00],\n",
       "       [1.6610199e-08, 9.8910505e-01, 1.0894936e-02, 2.2300015e-09],\n",
       "       [4.8509969e-07, 7.5353014e-01, 2.4646670e-01, 2.6560656e-06],\n",
       "       ...,\n",
       "       [2.1255244e-09, 9.9924743e-01, 7.5265008e-04, 1.9098740e-09],\n",
       "       [3.1215457e-15, 9.9994254e-01, 5.7482459e-05, 1.2886323e-12],\n",
       "       [1.8595629e-24, 2.3349529e-23, 1.3258132e-08, 1.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(K):\n",
    "    pred_cross[i] = pd.DataFrame(pred_cross[str(i)+'pred'])\n",
    "#pred_cross1 = pd.DataFrame(pred_cross['1pred'])\n",
    "#pred_cross2 = pd.DataFrame(pred_cross['2pred'])\n",
    "#pred_cross3 = pd.DataFrame(pred_cross['3pred'])\n",
    "#pred_cross4 = pd.DataFrame(pred_cross['4pred'])\n",
    "pred_cross['0pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_prob=[]\n",
    "for i in range(K):\n",
    "    y_pred.append([np.argmax(pred_cross[str(i)+'pred'][j]) for j in range(len(y_test))]) #  10 x 40000 array\n",
    "    y_prob.append([np.max(pred_cross[str(i)+'pred'][j]) for j in range(len(y_test))])    # 10 x 40000  array\n",
    "\n",
    "#y_pred_1 = [np.argmax(pred_cross['1pred'][j]) for j in range(len(y_test))]\n",
    "#y_pred_2 = [np.argmax(pred_cross['2pred'][j]) for j in range(len(y_test))]\n",
    "#y_pred_3 = [np.argmax(pred_cross['3pred'][j]) for j in range(len(y_test))]\n",
    "#y_pred_4 = [np.argmax(pred_cross['4pred'][j]) for j in range(len(y_test))]\n",
    "\n",
    "\n",
    "\n",
    "#y_prob_0 = [np.max(pred_cross['0pred'][j]) for j in range(len(y_test))]\n",
    "#y_prob_1 = [np.max(pred_cross['1pred'][j]) for j in range(len(y_test))]\n",
    "#y_prob_2 = [np.max(pred_cross['2pred'][j]) for j in range(len(y_test))]\n",
    "#y_prob_3 = [np.max(pred_cross['3pred'][j]) for j in range(len(y_test))]\n",
    "#y_prob_4 = [np.max(pred_cross['4pred'][j]) for j in range(len(y_test))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995627"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob[9][39999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_y_test = pd.DataFrame({'y_test':y_test})\n",
    "pd_y_prob = {}\n",
    "pd_y_pred = {}\n",
    "for i in range(K):\n",
    "    pd_y_prob['y_prob'+str(i)] = y_prob[i]\n",
    "    pd_y_pred['y_pred'+str(i)] = y_pred[i]\n",
    "    \n",
    "pd_y_prob = pd.DataFrame(pd_y_prob)\n",
    "pd_y_pred = pd.DataFrame(pd_y_pred)\n",
    "\n",
    "df_final = pd.concat([pd_y_test,pd_y_pred,pd_y_prob], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the DATA :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving df_final\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_out = open(\"K-fold-data-LEP.pickle\",\"wb\")\n",
    "pickle.dump(df_final, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred0</th>\n",
       "      <th>y_pred1</th>\n",
       "      <th>y_pred2</th>\n",
       "      <th>y_pred3</th>\n",
       "      <th>y_pred4</th>\n",
       "      <th>y_pred5</th>\n",
       "      <th>y_pred6</th>\n",
       "      <th>y_pred7</th>\n",
       "      <th>y_pred8</th>\n",
       "      <th>...</th>\n",
       "      <th>y_prob0</th>\n",
       "      <th>y_prob1</th>\n",
       "      <th>y_prob2</th>\n",
       "      <th>y_prob3</th>\n",
       "      <th>y_prob4</th>\n",
       "      <th>y_prob5</th>\n",
       "      <th>y_prob6</th>\n",
       "      <th>y_prob7</th>\n",
       "      <th>y_prob8</th>\n",
       "      <th>y_prob9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860830</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.999861</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989105</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925700</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.999970</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.997840</td>\n",
       "      <td>0.993454</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753530</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.997275</td>\n",
       "      <td>0.999379</td>\n",
       "      <td>0.999892</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999845</td>\n",
       "      <td>0.998638</td>\n",
       "      <td>0.999959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997814</td>\n",
       "      <td>0.999503</td>\n",
       "      <td>0.871085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.523190</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_test  y_pred0  y_pred1  y_pred2  y_pred3  y_pred4  y_pred5  y_pred6  \\\n",
       "0       2        3        1        3        3        3        1        3   \n",
       "1       0        1        3        3        0        2        1        1   \n",
       "2       3        1        2        0        0        2        0        3   \n",
       "3       0        3        3        1        2        0        2        0   \n",
       "4       0        0        2        3        0        3        3        3   \n",
       "\n",
       "   y_pred7  y_pred8    ...      y_prob0   y_prob1   y_prob2   y_prob3  \\\n",
       "0        0        2    ...     1.000000  0.999900  1.000000  0.860830   \n",
       "1        1        2    ...     0.989105  1.000000  0.925700  0.999999   \n",
       "2        0        2    ...     0.753530  0.999918  0.997275  0.999379   \n",
       "3        3        2    ...     1.000000  0.997814  0.999503  0.871085   \n",
       "4        2        2    ...     1.000000  0.998810  1.000000  0.999995   \n",
       "\n",
       "    y_prob4   y_prob5   y_prob6   y_prob7   y_prob8   y_prob9  \n",
       "0  0.999988  0.999978  1.000000  0.999959  0.999861  0.999983  \n",
       "1  0.999943  0.999970  0.999891  0.997840  0.993454  1.000000  \n",
       "2  0.999892  1.000000  1.000000  0.999845  0.998638  0.999959  \n",
       "3  1.000000  0.999933  0.999999  1.000000  0.523190  1.000000  \n",
       "4  1.000000  1.000000  0.999990  0.999959  0.999403  1.000000  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load\n",
    "\n",
    "pickle_in = open(\"K-fold-data-LEP.pickle\",\"rb\")\n",
    "df_f= pickle.load(pickle_in)\n",
    "df_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "K =10\n",
    "pd_y_test =pd.DataFrame( {'y_test':df_f['y_test']})\n",
    "y_pred = [df_f['y_pred'+str(i)] for i in range(K)]\n",
    "y_prob = [df_f['y_prob'+str(i)] for i in range(K)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets do first for the pred0 prob0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred0</th>\n",
       "      <th>y_prob0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.989105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_test  y_pred0   y_prob0\n",
       "0       2        3  1.000000\n",
       "1       0        1  0.989105\n",
       "2       3        1  0.753530\n",
       "3       0        3  1.000000\n",
       "4       0        0  1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_final00 = pd.concat([pd_y_test,pd.DataFrame({'y_pred0':y_pred[0]}),pd.DataFrame({'y_prob0':y_prob[0]})], axis=1)\n",
    "df_final00.head() #40000 test events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_final = []\n",
    "FN_final = []\n",
    "FP_final = []\n",
    "TN_final = []\n",
    "\n",
    "sh = 100\n",
    "sh0 = 0.9999\n",
    "\n",
    "threshold = [sh0 + (1-sh0)* i/(sh) for i in range(sh)]\n",
    "#print('threshold:',threshold)\n",
    "\n",
    "for th in threshold:\n",
    "    TP = []\n",
    "    FN = []\n",
    "    TN = []\n",
    "    FP = []\n",
    "    ct=0\n",
    "    ct1=0\n",
    "    ct2=0\n",
    "    ct3=0\n",
    "    ct4=0\n",
    "    ct5=0\n",
    "# true positive : A wolf appear and I see the wolf\n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct = df_final00[(df_final00.y_test ==i) & (df_final00.y_pred0 == i) & (df_final00.y_prob0 > th) ].count()\n",
    "        TP.append(ct.y_test)\n",
    "    TP_final.append(TP)\n",
    "# false negative: A wolf appear but I not see the wolf\n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct1 =df_final00[(df_final00.y_test ==i) & (df_final00.y_pred0 != i) & (df_final00.y_prob0 >th) ].count()\n",
    "        ct2= df_final00[(df_final00.y_test ==i) & (df_final00.y_pred0 == i) & (df_final00.y_prob0 <th) ].count()\n",
    "        FN.append(ct1.y_test + ct2.y_test)\n",
    "    FN_final.append(FN)\n",
    "# false positive: doesn't have a wolf but I see it \n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct3 =df_final00[(df_final00.y_test != i) & (df_final00.y_pred0 == i) & (df_final00.y_prob0 > th) ].count()\n",
    "        FP.append(ct3.y_test)\n",
    "    FP_final.append(FP)\n",
    "# true negative: doesn't have a wolf and I do not see it    \n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct4=df_final00[(df_final00.y_test != i) & (df_final00.y_pred0 != i)  & (df_final00.y_prob0 > th)].count()\n",
    "        ct5 =df_final00[(df_final00.y_test !=i) & (df_final00.y_pred0 == i) & (df_final00.y_prob0 <th) ].count()\n",
    "        TN.append(ct4.y_test+ct5.y_test)\n",
    "    TN_final.append(TN)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2348dc55470>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2348d9c21d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_lin = np.linspace(0.0, 1.0, sh)\n",
    "y0 =[[TP_final[i][j]/(TP_final[i][j]+FN_final[i][j]) for i in range(sh)]for j in range(len(CATEGORIES))]\n",
    "x0 = [[FP_final[i][j]/(FP_final[i][j]+ TN_final[i][j]) for i in range(sh)]for j in range(len(CATEGORIES))]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_lin, x_lin , '--',color = 'gray',)\n",
    "for j in range(len(CATEGORIES)):\n",
    "    plt.scatter(x0[j],y0[j],label= CATEGORIES[j])\n",
    "    plt.legend(loc='lower right',fontsize=10)\n",
    "plt.xlabel('FPR%')\n",
    "plt.ylabel('TPR%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred1</th>\n",
       "      <th>y_prob1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.997814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_test  y_pred1   y_prob1\n",
       "0       2        1  0.999900\n",
       "1       0        3  1.000000\n",
       "2       3        2  0.999918\n",
       "3       0        3  0.997814\n",
       "4       0        2  0.998810"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lets do for the pred1 prob1\n",
    "\n",
    "df_final11 = pd.concat([pd_y_test,pd.DataFrame({'y_pred1':y_pred[1]}),pd.DataFrame({'y_prob1':y_prob[1]})], axis=1)\n",
    "df_final11.head() #40000 test events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_final = []\n",
    "FN_final = []\n",
    "FP_final = []\n",
    "TN_final = []\n",
    "\n",
    "#sh = 40\n",
    "#sh0 = 0.999\n",
    "\n",
    "threshold = [sh0 + (1-sh0)* i/(sh) for i in range(sh)]\n",
    "#print('threshold:',threshold)\n",
    "\n",
    "for th in threshold:\n",
    "    TP = []\n",
    "    FN = []\n",
    "    TN = []\n",
    "    FP = []\n",
    "    ct=0\n",
    "    ct1=0\n",
    "    ct2=0\n",
    "    ct3=0\n",
    "    ct4=0\n",
    "    ct5=0\n",
    "# true positive : A wolf appear and I see the wolf\n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct = df_final11[(df_final11.y_test ==i) & (df_final11.y_pred1 == i) & (df_final11.y_prob1 > th) ].count()\n",
    "        TP.append(ct.y_test)\n",
    "    TP_final.append(TP)\n",
    "# false negative: A wolf appear but I not see the wolf\n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct1 =df_final11[(df_final11.y_test ==i) & (df_final11.y_pred1 != i) & (df_final11.y_prob1 >th) ].count()\n",
    "        ct2= df_final11[(df_final11.y_test ==i) & (df_final11.y_pred1 == i) & (df_final11.y_prob1 <th) ].count()\n",
    "        FN.append(ct1.y_test + ct2.y_test)\n",
    "    FN_final.append(FN)\n",
    "# false positive: doesn't have a wolf but I see it \n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct3 =df_final11[(df_final11.y_test != i) & (df_final11.y_pred1 == i) & (df_final11.y_prob1 > th) ].count()\n",
    "        FP.append(ct3.y_test)\n",
    "    FP_final.append(FP)\n",
    "# true negative: doesn't have a wolf and I do not see it    \n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct4=df_final11[(df_final11.y_test != i) & (df_final11.y_pred1 != i)  & (df_final11.y_prob1 > th)].count()\n",
    "        ct5 =df_final11[(df_final11.y_test !=i) & (df_final11.y_pred1 == i) & (df_final11.y_prob1 <th) ].count()\n",
    "        TN.append(ct4.y_test+ct5.y_test)\n",
    "    TN_final.append(TN)\n",
    "    \n",
    "\n",
    "#x_lin = np.linspace(0.0, 1.0, sh)\n",
    "y1 =[[TP_final[i][j]/(TP_final[i][j]+FN_final[i][j]) for i in range(sh)]for j in range(len(CATEGORIES))]\n",
    "x1 = [[FP_final[i][j]/(FP_final[i][j]+ TN_final[i][j]) for i in range(sh)]for j in range(len(CATEGORIES))]\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.plot(x_lin, x_lin , '--',color = 'gray',)\n",
    "#for j in range(len(CATEGORIES)):\n",
    "#    plt.scatter(x1[j],y1[j],label= CATEGORIES[j])\n",
    "#    plt.legend(loc='lower right',fontsize=10)\n",
    "#plt.xlabel('FPR%')\n",
    "#plt.ylabel('TPR%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred2</th>\n",
       "      <th>y_prob2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_test  y_pred2   y_prob2\n",
       "0       2        3  1.000000\n",
       "1       0        3  0.925700\n",
       "2       3        0  0.997275\n",
       "3       0        1  0.999503\n",
       "4       0        3  1.000000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lets do for the pred2 prob2\n",
    "\n",
    "df_final22 = pd.concat([pd_y_test,pd.DataFrame({'y_pred2':y_pred[2]}),pd.DataFrame({'y_prob2':y_prob[2]})], axis=1)\n",
    "df_final22.head() #40000 test events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_final = []\n",
    "FN_final = []\n",
    "FP_final = []\n",
    "TN_final = []\n",
    "\n",
    "#sh = 40\n",
    "#sh0 = 0.999\n",
    "\n",
    "threshold = [sh0 + (1-sh0)* i/(sh) for i in range(sh)]\n",
    "#print('threshold:',threshold)\n",
    "\n",
    "for th in threshold:\n",
    "    TP = []\n",
    "    FN = []\n",
    "    TN = []\n",
    "    FP = []\n",
    "    ct=0\n",
    "    ct1=0\n",
    "    ct2=0\n",
    "    ct3=0\n",
    "    ct4=0\n",
    "    ct5=0\n",
    "# true positive : A wolf appear and I see the wolf\n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct = df_final22[(df_final22.y_test ==i) & (df_final22.y_pred2 == i) & (df_final22.y_prob2 > th) ].count()\n",
    "        TP.append(ct.y_test)\n",
    "    TP_final.append(TP)\n",
    "# false negative: A wolf appear but I not see the wolf\n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct1 =df_final22[(df_final22.y_test ==i) & (df_final22.y_pred2 != i) & (df_final22.y_prob2 >th) ].count()\n",
    "        ct2= df_final22[(df_final22.y_test ==i) & (df_final22.y_pred2 == i) & (df_final22.y_prob2 <th) ].count()\n",
    "        FN.append(ct1.y_test + ct2.y_test)\n",
    "    FN_final.append(FN)\n",
    "# false positive: doesn't have a wolf but I see it \n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct3 =df_final22[(df_final22.y_test != i) & (df_final22.y_pred2 == i) & (df_final22.y_prob2 > th) ].count()\n",
    "        FP.append(ct3.y_test)\n",
    "    FP_final.append(FP)\n",
    "# true negative: doesn't have a wolf and I do not see it    \n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct4=df_final22[(df_final22.y_test != i) & (df_final22.y_pred2 != i)  & (df_final22.y_prob2 > th)].count()\n",
    "        ct5 =df_final22[(df_final22.y_test !=i) & (df_final22.y_pred2 == i) & (df_final22.y_prob2 <th) ].count()\n",
    "        TN.append(ct4.y_test+ct5.y_test)\n",
    "    TN_final.append(TN)\n",
    "    \n",
    "\n",
    "x_lin = np.linspace(0.0, 1.0, sh)\n",
    "y2 =[[TP_final[i][j]/(TP_final[i][j]+FN_final[i][j]) for i in range(sh)]for j in range(len(CATEGORIES))]\n",
    "x2 = [[FP_final[i][j]/(FP_final[i][j]+ TN_final[i][j]) for i in range(sh)]for j in range(len(CATEGORIES))]\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.plot(x_lin, x_lin , '--',color = 'gray',)\n",
    "#for j in range(len(CATEGORIES)):\n",
    "#    plt.scatter(x2[j],y2[j],label= CATEGORIES[j])\n",
    "#    plt.legend(loc='lower right',fontsize=10)\n",
    "#plt.xlabel('FPR%')\n",
    "#plt.ylabel('TPR%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred3</th>\n",
       "      <th>y_prob3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.860830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.871085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_test  y_pred3   y_prob3\n",
       "0       2        3  0.860830\n",
       "1       0        0  0.999999\n",
       "2       3        0  0.999379\n",
       "3       0        2  0.871085\n",
       "4       0        0  0.999995"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lets do for the pred2 prob2\n",
    "\n",
    "df_final33 = pd.concat([pd_y_test,pd.DataFrame({'y_pred3':y_pred[3]}),pd.DataFrame({'y_prob3':y_prob[3]})], axis=1)\n",
    "df_final33.head() #40000 test events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_final = []\n",
    "FN_final = []\n",
    "FP_final = []\n",
    "TN_final = []\n",
    "\n",
    "#sh = 40\n",
    "#sh0 = 0.999\n",
    "\n",
    "threshold = [sh0 + (1-sh0)* i/(sh) for i in range(sh)]\n",
    "#print('threshold:',threshold)\n",
    "\n",
    "for th in threshold:\n",
    "    TP = []\n",
    "    FN = []\n",
    "    TN = []\n",
    "    FP = []\n",
    "    ct=0\n",
    "    ct1=0\n",
    "    ct2=0\n",
    "    ct3=0\n",
    "    ct4=0\n",
    "    ct5=0\n",
    "# true positive : A wolf appear and I see the wolf\n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct = df_final33[(df_final33.y_test ==i) & (df_final33.y_pred3 == i) & (df_final33.y_prob3 > th) ].count()\n",
    "        TP.append(ct.y_test)\n",
    "    TP_final.append(TP)\n",
    "# false negative: A wolf appear but I not see the wolf\n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct1 =df_final33[(df_final33.y_test ==i) & (df_final33.y_pred3 != i) & (df_final33.y_prob3 >th) ].count()\n",
    "        ct2= df_final33[(df_final33.y_test ==i) & (df_final33.y_pred3 == i) & (df_final33.y_prob3 <th) ].count()\n",
    "        FN.append(ct1.y_test + ct2.y_test)\n",
    "    FN_final.append(FN)\n",
    "# false positive: doesn't have a wolf but I see it \n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct3 =df_final33[(df_final33.y_test != i) & (df_final33.y_pred3 == i) & (df_final33.y_prob3 > th) ].count()\n",
    "        FP.append(ct3.y_test)\n",
    "    FP_final.append(FP)\n",
    "# true negative: doesn't have a wolf and I do not see it    \n",
    "    for i in range(len(CATEGORIES)):\n",
    "        ct4=df_final33[(df_final33.y_test != i) & (df_final33.y_pred3 != i)  & (df_final33.y_prob3 > th)].count()\n",
    "        ct5 =df_final33[(df_final33.y_test !=i) & (df_final33.y_pred3 == i) & (df_final33.y_prob3 <th) ].count()\n",
    "        TN.append(ct4.y_test+ct5.y_test)\n",
    "    TN_final.append(TN)\n",
    "    \n",
    "\n",
    "#x_lin = np.linspace(0.0, 1.0, sh)\n",
    "y3 =[[TP_final[i][j]/(TP_final[i][j]+FN_final[i][j]) for i in range(sh)]for j in range(len(CATEGORIES))]\n",
    "x3 = [[FP_final[i][j]/(FP_final[i][j]+ TN_final[i][j]) for i in range(sh)]for j in range(len(CATEGORIES))]\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.plot(x_lin, x_lin , '--',color = 'gray',)\n",
    "#for j in range(len(CATEGORIES)):\n",
    "#    plt.scatter(x3[j],y3[j],label= CATEGORIES[j])\n",
    "#    plt.legend(loc='lower right',fontsize=10)\n",
    "#plt.xlabel('FPR%')\n",
    "#plt.ylabel('TPR%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "4 4\n"
     ]
    }
   ],
   "source": [
    "# Average\n",
    "\n",
    "#y[3][39]\n",
    "y = [y0,y1,y2,y3]\n",
    "x = [x0,x1,x2,x3]#K=4 x CAT=4 X SH=40\n",
    "TPR_ave = []\n",
    "FPR_ave = []\n",
    "\n",
    "for j in range(len(CATEGORIES)):\n",
    "    ttt = []\n",
    "    qqq =[]\n",
    "    for k in range(sh):\n",
    "        ttt.append(np.average([y[i][j][k] for i in range(3)]))\n",
    "        qqq.append(np.average([x[i][j][k] for i in range(3)]))\n",
    "    TPR_ave.append(ttt)\n",
    "    FPR_ave.append(qqq)\n",
    "print(len(TPR_ave),len(FPR_ave))\n",
    "\n",
    "\n",
    "\n",
    "# Standard deviation\n",
    "\n",
    "TPR_std = []\n",
    "FPR_std = []\n",
    "\n",
    "for j in range(len(CATEGORIES)):\n",
    "    ttt = []\n",
    "    qqq =[]\n",
    "    for k in range(sh):\n",
    "        ttt.append(np.std([y[i][j][k] for i in range(3)]))\n",
    "        qqq.append(np.std([x[i][j][k] for i in range(3)]))\n",
    "    TPR_std.append(ttt)\n",
    "    FPR_std.append(qqq)\n",
    "print(len(TPR_std),len(FPR_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:2922: MatplotlibDeprecationWarning: The 'hold' keyword argument is deprecated since 2.0.\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2348dc7ed30>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npZSbhoJSSim3/w+y0/y15s4REgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2348dd15da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_lin, x_lin , '--',color = 'gray',)\n",
    "for j in range(len(CATEGORIES)):\n",
    "    plt.scatter(TPR_ave[j],FPR_ave[j],label= CATEGORIES[j])\n",
    "    plt.errorbar(TPR_ave[j],FPR_ave[j], yerr=TPR_std[j], xerr=FPR_std[j], hold=True, fmt='none')\n",
    "    plt.legend(loc='lower right',fontsize=10)\n",
    "plt.xlabel('FPR%')\n",
    "plt.ylabel('TPR%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
